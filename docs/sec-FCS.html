<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="Flexible Imputation of Missing Data, Second Edition">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Flexible Imputation of Missing Data, Second Edition" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  <meta name="twitter:description" content="Flexible Imputation of Missing Data, Second Edition" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="sec-JM.html">
<link rel="next" href="fcs-and-jm.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://stefvanbuuren.name/fimd/"> Flexible Imputation of Missing Data</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="want-the-hardcopy.html"><a href="want-the-hardcopy.html"><i class="fa fa-check"></i>Want the hardcopy?</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="" data-path="preface-to-second-edition.html"><a href="preface-to-second-edition.html"><i class="fa fa-check"></i>Preface to second edition</a></li>
<li class="chapter" data-level="" data-path="preface-to-first-edition.html"><a href="preface-to-first-edition.html"><i class="fa fa-check"></i>Preface to first edition</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="symbol-description.html"><a href="symbol-description.html"><i class="fa fa-check"></i>Symbol Description</a></li>
<li class="part"><span><b>I Part I: Basics</b></span></li>
<li class="chapter" data-level="1" data-path="ch-introduction.html"><a href="ch-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="sec-problem.html"><a href="sec-problem.html"><i class="fa fa-check"></i><b>1.1</b> The problem of missing data</a><ul>
<li class="chapter" data-level="1.1.1" data-path="sec-problem.html"><a href="sec-problem.html#sec:current"><i class="fa fa-check"></i><b>1.1.1</b> Current practice</a></li>
<li class="chapter" data-level="1.1.2" data-path="sec-problem.html"><a href="sec-problem.html#sec:changingperspective"><i class="fa fa-check"></i><b>1.1.2</b> Changing perspective on missing data</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="sec-MCAR.html"><a href="sec-MCAR.html"><i class="fa fa-check"></i><b>1.2</b> Concepts of MCAR, MAR and MNAR</a></li>
<li class="chapter" data-level="1.3" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html"><i class="fa fa-check"></i><b>1.3</b> Ad-hoc solutions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:listwise"><i class="fa fa-check"></i><b>1.3.1</b> Listwise deletion</a></li>
<li class="chapter" data-level="1.3.2" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:pairwise"><i class="fa fa-check"></i><b>1.3.2</b> Pairwise deletion</a></li>
<li class="chapter" data-level="1.3.3" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:meanimp"><i class="fa fa-check"></i><b>1.3.3</b> Mean imputation</a></li>
<li class="chapter" data-level="1.3.4" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:regimp"><i class="fa fa-check"></i><b>1.3.4</b> Regression imputation</a></li>
<li class="chapter" data-level="1.3.5" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:sri"><i class="fa fa-check"></i><b>1.3.5</b> Stochastic regression imputation</a></li>
<li class="chapter" data-level="1.3.6" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:locf"><i class="fa fa-check"></i><b>1.3.6</b> LOCF and BOCF</a></li>
<li class="chapter" data-level="1.3.7" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:indicator"><i class="fa fa-check"></i><b>1.3.7</b> Indicator method</a></li>
<li class="chapter" data-level="1.3.8" data-path="sec-simplesolutions.html"><a href="sec-simplesolutions.html#sec:simplesummary"><i class="fa fa-check"></i><b>1.3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="sec-nutshell.html"><a href="sec-nutshell.html"><i class="fa fa-check"></i><b>1.4</b> Multiple imputation in a nutshell</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-nutshell.html"><a href="sec-nutshell.html#procedure"><i class="fa fa-check"></i><b>1.4.1</b> Procedure</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-nutshell.html"><a href="sec-nutshell.html#reasons-to-use-multiple-imputation"><i class="fa fa-check"></i><b>1.4.2</b> Reasons to use multiple imputation</a></li>
<li class="chapter" data-level="1.4.3" data-path="sec-nutshell.html"><a href="sec-nutshell.html#sec:miexample"><i class="fa fa-check"></i><b>1.4.3</b> Example of multiple imputation</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="sec-goal.html"><a href="sec-goal.html"><i class="fa fa-check"></i><b>1.5</b> Goal of the book</a></li>
<li class="chapter" data-level="1.6" data-path="sec-doesnotcover.html"><a href="sec-doesnotcover.html"><i class="fa fa-check"></i><b>1.6</b> What the book does not cover</a><ul>
<li class="chapter" data-level="1.6.1" data-path="sec-doesnotcover.html"><a href="sec-doesnotcover.html#sec:prevention"><i class="fa fa-check"></i><b>1.6.1</b> Prevention</a></li>
<li class="chapter" data-level="1.6.2" data-path="sec-doesnotcover.html"><a href="sec-doesnotcover.html#sec:weighting"><i class="fa fa-check"></i><b>1.6.2</b> Weighting procedures</a></li>
<li class="chapter" data-level="1.6.3" data-path="sec-doesnotcover.html"><a href="sec-doesnotcover.html#sec:likelihood"><i class="fa fa-check"></i><b>1.6.3</b> Likelihood-based approaches</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-structure.html"><a href="sec-structure.html"><i class="fa fa-check"></i><b>1.7</b> Structure of the book</a></li>
<li class="chapter" data-level="1.8" data-path="ex-ch1.html"><a href="ex-ch1.html"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-mi.html"><a href="ch-mi.html"><i class="fa fa-check"></i><b>2</b> Multiple imputation</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-historic.html"><a href="sec-historic.html"><i class="fa fa-check"></i><b>2.1</b> Historic overview</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-historic.html"><a href="sec-historic.html#imputation"><i class="fa fa-check"></i><b>2.1.1</b> Imputation</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-historic.html"><a href="sec-historic.html#multiple-imputation"><i class="fa fa-check"></i><b>2.1.2</b> Multiple imputation</a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-historic.html"><a href="sec-historic.html#the-expanding-literature-on-multiple-imputation"><i class="fa fa-check"></i><b>2.1.3</b> The expanding literature on multiple imputation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html"><i class="fa fa-check"></i><b>2.2</b> Concepts in incomplete data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html#incomplete-data-perspective"><i class="fa fa-check"></i><b>2.2.1</b> Incomplete-data perspective</a></li>
<li class="chapter" data-level="2.2.2" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html#causes-of-missing-data"><i class="fa fa-check"></i><b>2.2.2</b> Causes of missing data</a></li>
<li class="chapter" data-level="2.2.3" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html#sec:notation"><i class="fa fa-check"></i><b>2.2.3</b> Notation</a></li>
<li class="chapter" data-level="2.2.4" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html#sec:MCARreprise"><i class="fa fa-check"></i><b>2.2.4</b> MCAR, MAR and MNAR again</a></li>
<li class="chapter" data-level="2.2.5" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html#sec:ignorable"><i class="fa fa-check"></i><b>2.2.5</b> Ignorable and nonignorable<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="2.2.6" data-path="sec-idconcepts.html"><a href="sec-idconcepts.html#sec:ignorability"><i class="fa fa-check"></i><b>2.2.6</b> Implications of ignorability</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html"><i class="fa fa-check"></i><b>2.3</b> Why and when multiple imputation works</a><ul>
<li class="chapter" data-level="2.3.1" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#sec:migoal"><i class="fa fa-check"></i><b>2.3.1</b> Goal of multiple imputation</a></li>
<li class="chapter" data-level="2.3.2" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#sec:threesources"><i class="fa fa-check"></i><b>2.3.2</b> Three sources of variation<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#sec:proper"><i class="fa fa-check"></i><b>2.3.3</b> Proper imputation</a></li>
<li class="chapter" data-level="2.3.4" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#scope-of-the-imputation-model"><i class="fa fa-check"></i><b>2.3.4</b> Scope of the imputation model</a></li>
<li class="chapter" data-level="2.3.5" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#sec:varianceratios"><i class="fa fa-check"></i><b>2.3.5</b> Variance ratios<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="2.3.6" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#sec:df"><i class="fa fa-check"></i><b>2.3.6</b> Degrees of freedom<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="2.3.7" data-path="sec-whyandwhen.html"><a href="sec-whyandwhen.html#numerical-example"><i class="fa fa-check"></i><b>2.3.7</b> Numerical example</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="sec-inference.html"><a href="sec-inference.html"><i class="fa fa-check"></i><b>2.4</b> Statistical intervals and tests</a><ul>
<li class="chapter" data-level="2.4.1" data-path="sec-inference.html"><a href="sec-inference.html#scalar-or-multi-parameter-inference"><i class="fa fa-check"></i><b>2.4.1</b> Scalar or multi-parameter inference?</a></li>
<li class="chapter" data-level="2.4.2" data-path="sec-inference.html"><a href="sec-inference.html#sec:singlepar"><i class="fa fa-check"></i><b>2.4.2</b> Scalar inference</a></li>
<li class="chapter" data-level="2.4.3" data-path="sec-inference.html"><a href="sec-inference.html#numerical-example-1"><i class="fa fa-check"></i><b>2.4.3</b> Numerical example</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="sec-evaluation.html"><a href="sec-evaluation.html"><i class="fa fa-check"></i><b>2.5</b> How to evaluate imputation methods</a><ul>
<li class="chapter" data-level="2.5.1" data-path="sec-evaluation.html"><a href="sec-evaluation.html#simulation-designs-and-performance-measures"><i class="fa fa-check"></i><b>2.5.1</b> Simulation designs and performance measures</a></li>
<li class="chapter" data-level="2.5.2" data-path="sec-evaluation.html"><a href="sec-evaluation.html#sec:evaluationcriteria"><i class="fa fa-check"></i><b>2.5.2</b> Evaluation criteria</a></li>
<li class="chapter" data-level="2.5.3" data-path="sec-evaluation.html"><a href="sec-evaluation.html#sec:quantifyingbias"><i class="fa fa-check"></i><b>2.5.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="sec-true.html"><a href="sec-true.html"><i class="fa fa-check"></i><b>2.6</b> Imputation is not prediction</a></li>
<li class="chapter" data-level="2.7" data-path="sec-when.html"><a href="sec-when.html"><i class="fa fa-check"></i><b>2.7</b> When not to use multiple imputation</a></li>
<li class="chapter" data-level="2.8" data-path="sec-howmany.html"><a href="sec-howmany.html"><i class="fa fa-check"></i><b>2.8</b> How many imputations?</a></li>
<li class="chapter" data-level="2.9" data-path="sec-exmi.html"><a href="sec-exmi.html"><i class="fa fa-check"></i><b>2.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-univariate.html"><a href="ch-univariate.html"><i class="fa fa-check"></i><b>3</b> Univariate missing data</a><ul>
<li class="chapter" data-level="3.1" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html"><i class="fa fa-check"></i><b>3.1</b> How to generate multiple imputations</a><ul>
<li class="chapter" data-level="3.1.1" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html#predict-method"><i class="fa fa-check"></i><b>3.1.1</b> Predict method</a></li>
<li class="chapter" data-level="3.1.2" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html#sec:meth2"><i class="fa fa-check"></i><b>3.1.2</b> Predict + noise method</a></li>
<li class="chapter" data-level="3.1.3" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html#sec:meth3"><i class="fa fa-check"></i><b>3.1.3</b> Predict + noise + parameter uncertainty</a></li>
<li class="chapter" data-level="3.1.4" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html#a-second-predictor"><i class="fa fa-check"></i><b>3.1.4</b> A second predictor</a></li>
<li class="chapter" data-level="3.1.5" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html#drawing-from-the-observed-data"><i class="fa fa-check"></i><b>3.1.5</b> Drawing from the observed data</a></li>
<li class="chapter" data-level="3.1.6" data-path="how-to-generate-multiple-imputations.html"><a href="how-to-generate-multiple-imputations.html#conclusion"><i class="fa fa-check"></i><b>3.1.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html"><i class="fa fa-check"></i><b>3.2</b> Imputation under the normal linear normal</a><ul>
<li class="chapter" data-level="3.2.1" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html#sec:linearoverview"><i class="fa fa-check"></i><b>3.2.1</b> Overview</a></li>
<li class="chapter" data-level="3.2.2" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html#sec:linearalgorithm"><i class="fa fa-check"></i><b>3.2.2</b> Algorithms<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html#sec:perflin"><i class="fa fa-check"></i><b>3.2.3</b> Performance</a></li>
<li class="chapter" data-level="3.2.4" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html#sec:generateuni"><i class="fa fa-check"></i><b>3.2.4</b> Generating MAR missing data</a></li>
<li class="chapter" data-level="3.2.5" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html#sec:generatemulti"><i class="fa fa-check"></i><b>3.2.5</b> MAR missing data generation in multivariate data</a></li>
<li class="chapter" data-level="3.2.6" data-path="sec-linearnormal.html"><a href="sec-linearnormal.html#conclusion-1"><i class="fa fa-check"></i><b>3.2.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sec-nonnormal.html"><a href="sec-nonnormal.html"><i class="fa fa-check"></i><b>3.3</b> Imputation under non-normal distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-nonnormal.html"><a href="sec-nonnormal.html#overview"><i class="fa fa-check"></i><b>3.3.1</b> Overview</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-nonnormal.html"><a href="sec-nonnormal.html#sec:tdist"><i class="fa fa-check"></i><b>3.3.2</b> Imputation from the <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-pmm.html"><a href="sec-pmm.html"><i class="fa fa-check"></i><b>3.4</b> Predictive mean matching</a><ul>
<li class="chapter" data-level="3.4.1" data-path="sec-pmm.html"><a href="sec-pmm.html#overview-1"><i class="fa fa-check"></i><b>3.4.1</b> Overview</a></li>
<li class="chapter" data-level="3.4.2" data-path="sec-pmm.html"><a href="sec-pmm.html#sec:pmmcomputation"><i class="fa fa-check"></i><b>3.4.2</b> Computational details<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="3.4.3" data-path="sec-pmm.html"><a href="sec-pmm.html#number-of-donors"><i class="fa fa-check"></i><b>3.4.3</b> Number of donors</a></li>
<li class="chapter" data-level="3.4.4" data-path="sec-pmm.html"><a href="sec-pmm.html#pitfalls"><i class="fa fa-check"></i><b>3.4.4</b> Pitfalls</a></li>
<li class="chapter" data-level="3.4.5" data-path="sec-pmm.html"><a href="sec-pmm.html#conclusion-2"><i class="fa fa-check"></i><b>3.4.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="sec-cart.html"><a href="sec-cart.html"><i class="fa fa-check"></i><b>3.5</b> Classification and regression trees</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-cart.html"><a href="sec-cart.html#sec:cartoverview"><i class="fa fa-check"></i><b>3.5.1</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="sec-categorical.html"><a href="sec-categorical.html"><i class="fa fa-check"></i><b>3.6</b> Categorical data</a><ul>
<li class="chapter" data-level="3.6.1" data-path="sec-categorical.html"><a href="sec-categorical.html#sec:categoricaloverview"><i class="fa fa-check"></i><b>3.6.1</b> Generalized linear model</a></li>
<li class="chapter" data-level="3.6.2" data-path="sec-categorical.html"><a href="sec-categorical.html#perfect-predictionspadesuit"><i class="fa fa-check"></i><b>3.6.2</b> Perfect prediction<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="3.6.3" data-path="sec-categorical.html"><a href="sec-categorical.html#evaluation"><i class="fa fa-check"></i><b>3.6.3</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="other-data-types.html"><a href="other-data-types.html"><i class="fa fa-check"></i><b>3.7</b> Other data types</a><ul>
<li class="chapter" data-level="3.7.1" data-path="other-data-types.html"><a href="other-data-types.html#sec:count"><i class="fa fa-check"></i><b>3.7.1</b> Count data</a></li>
<li class="chapter" data-level="3.7.2" data-path="other-data-types.html"><a href="other-data-types.html#sec:semi"><i class="fa fa-check"></i><b>3.7.2</b> Semi-continuous data</a></li>
<li class="chapter" data-level="3.7.3" data-path="other-data-types.html"><a href="other-data-types.html#sec:censored"><i class="fa fa-check"></i><b>3.7.3</b> Censored, truncated and rounded data</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html"><i class="fa fa-check"></i><b>3.8</b> Nonignorable missing data</a><ul>
<li class="chapter" data-level="3.8.1" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#sec:nonignorableoverview"><i class="fa fa-check"></i><b>3.8.1</b> Overview</a></li>
<li class="chapter" data-level="3.8.2" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#sec:selectionmodel"><i class="fa fa-check"></i><b>3.8.2</b> Selection model</a></li>
<li class="chapter" data-level="3.8.3" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#sec:patternmixturemodel"><i class="fa fa-check"></i><b>3.8.3</b> Pattern-mixture model</a></li>
<li class="chapter" data-level="3.8.4" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#sec:convert"><i class="fa fa-check"></i><b>3.8.4</b> Converting selection and pattern-mixture models</a></li>
<li class="chapter" data-level="3.8.5" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#sec:ch3sensitivity"><i class="fa fa-check"></i><b>3.8.5</b> Sensitivity analysis</a></li>
<li class="chapter" data-level="3.8.6" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#role-of-sensitivity-analysis"><i class="fa fa-check"></i><b>3.8.6</b> Role of sensitivity analysis</a></li>
<li class="chapter" data-level="3.8.7" data-path="sec-nonignorable.html"><a href="sec-nonignorable.html#recent-developments"><i class="fa fa-check"></i><b>3.8.7</b> Recent developments</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="ex-ch-univariate.html"><a href="ex-ch-univariate.html"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-multivariate.html"><a href="ch-multivariate.html"><i class="fa fa-check"></i><b>4</b> Multivariate missing data</a><ul>
<li class="chapter" data-level="4.1" data-path="missing-data-pattern.html"><a href="missing-data-pattern.html"><i class="fa fa-check"></i><b>4.1</b> Missing data pattern</a><ul>
<li class="chapter" data-level="4.1.1" data-path="missing-data-pattern.html"><a href="missing-data-pattern.html#sec:patternoverview"><i class="fa fa-check"></i><b>4.1.1</b> Overview</a></li>
<li class="chapter" data-level="4.1.2" data-path="missing-data-pattern.html"><a href="missing-data-pattern.html#sec:mdpattern"><i class="fa fa-check"></i><b>4.1.2</b> Summary statistics</a></li>
<li class="chapter" data-level="4.1.3" data-path="missing-data-pattern.html"><a href="missing-data-pattern.html#sec:flux"><i class="fa fa-check"></i><b>4.1.3</b> Influx and outflux</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-issues.html"><a href="sec-issues.html"><i class="fa fa-check"></i><b>4.2</b> Issues in multivariate imputation</a></li>
<li class="chapter" data-level="4.3" data-path="sec-monotone.html"><a href="sec-monotone.html"><i class="fa fa-check"></i><b>4.3</b> Monotone data imputation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-monotone.html"><a href="sec-monotone.html#sec:monoverview"><i class="fa fa-check"></i><b>4.3.1</b> Overview</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-monotone.html"><a href="sec-monotone.html#sec:monalgorithm"><i class="fa fa-check"></i><b>4.3.2</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sec-JM.html"><a href="sec-JM.html"><i class="fa fa-check"></i><b>4.4</b> Joint modeling</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sec-JM.html"><a href="sec-JM.html#overview-2"><i class="fa fa-check"></i><b>4.4.1</b> Overview</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-JM.html"><a href="sec-JM.html#continuous-data"><i class="fa fa-check"></i><b>4.4.2</b> Continuous data</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-JM.html"><a href="sec-JM.html#sec:jmcategorical"><i class="fa fa-check"></i><b>4.4.3</b> Categorical data</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sec-FCS.html"><a href="sec-FCS.html"><i class="fa fa-check"></i><b>4.5</b> Fully conditional specification</a><ul>
<li class="chapter" data-level="4.5.1" data-path="sec-FCS.html"><a href="sec-FCS.html#overview-3"><i class="fa fa-check"></i><b>4.5.1</b> Overview</a></li>
<li class="chapter" data-level="4.5.2" data-path="sec-FCS.html"><a href="sec-FCS.html#sec:MICE"><i class="fa fa-check"></i><b>4.5.2</b> The MICE algorithm</a></li>
<li class="chapter" data-level="4.5.3" data-path="sec-FCS.html"><a href="sec-FCS.html#sec:compatibility"><i class="fa fa-check"></i><b>4.5.3</b> Compatibility<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="4.5.4" data-path="sec-FCS.html"><a href="sec-FCS.html#sec:congeniality"><i class="fa fa-check"></i><b>4.5.4</b> Congeniality or compatibility?</a></li>
<li class="chapter" data-level="4.5.5" data-path="sec-FCS.html"><a href="sec-FCS.html#sec:modelbased"><i class="fa fa-check"></i><b>4.5.5</b> Model-based and data-based imputation</a></li>
<li class="chapter" data-level="4.5.6" data-path="sec-FCS.html"><a href="sec-FCS.html#sec:howlarget"><i class="fa fa-check"></i><b>4.5.6</b> Number of iterations</a></li>
<li class="chapter" data-level="4.5.7" data-path="sec-FCS.html"><a href="sec-FCS.html#sec:slowconvergence"><i class="fa fa-check"></i><b>4.5.7</b> Example of slow convergence</a></li>
<li class="chapter" data-level="4.5.8" data-path="sec-FCS.html"><a href="sec-FCS.html#performance"><i class="fa fa-check"></i><b>4.5.8</b> Performance</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="fcs-and-jm.html"><a href="fcs-and-jm.html"><i class="fa fa-check"></i><b>4.6</b> FCS and JM</a><ul>
<li class="chapter" data-level="4.6.1" data-path="fcs-and-jm.html"><a href="fcs-and-jm.html#relations-between-fcs-and-jm"><i class="fa fa-check"></i><b>4.6.1</b> Relations between FCS and JM</a></li>
<li class="chapter" data-level="4.6.2" data-path="fcs-and-jm.html"><a href="fcs-and-jm.html#comparisons"><i class="fa fa-check"></i><b>4.6.2</b> Comparisons</a></li>
<li class="chapter" data-level="4.6.3" data-path="fcs-and-jm.html"><a href="fcs-and-jm.html#illustration"><i class="fa fa-check"></i><b>4.6.3</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="mice-extensions.html"><a href="mice-extensions.html"><i class="fa fa-check"></i><b>4.7</b> MICE extensions</a><ul>
<li class="chapter" data-level="4.7.1" data-path="mice-extensions.html"><a href="mice-extensions.html#skipping-imputations-and-overimputation"><i class="fa fa-check"></i><b>4.7.1</b> Skipping imputations and overimputation</a></li>
<li class="chapter" data-level="4.7.2" data-path="mice-extensions.html"><a href="mice-extensions.html#sec:blockvar"><i class="fa fa-check"></i><b>4.7.2</b> Blocks of variables, hybrid imputation</a></li>
<li class="chapter" data-level="4.7.3" data-path="mice-extensions.html"><a href="mice-extensions.html#sec:blockunit"><i class="fa fa-check"></i><b>4.7.3</b> Blocks of units, monotone blocks</a></li>
<li class="chapter" data-level="4.7.4" data-path="mice-extensions.html"><a href="mice-extensions.html#sec:tile"><i class="fa fa-check"></i><b>4.7.4</b> Tile imputation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="conclusion-3.html"><a href="conclusion-3.html"><i class="fa fa-check"></i><b>4.8</b> Conclusion</a></li>
<li class="chapter" data-level="4.9" data-path="ex-ch-multivariate.html"><a href="ex-ch-multivariate.html"><i class="fa fa-check"></i><b>4.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-analysis.html"><a href="ch-analysis.html"><i class="fa fa-check"></i><b>5</b> Analysis of imputed data</a><ul>
<li class="chapter" data-level="5.1" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>5.1</b> Workflow</a><ul>
<li class="chapter" data-level="5.1.1" data-path="workflow.html"><a href="workflow.html#sec:goodworkflows"><i class="fa fa-check"></i><b>5.1.1</b> Recommended workflows</a></li>
<li class="chapter" data-level="5.1.2" data-path="workflow.html"><a href="workflow.html#sec:badworkflowa"><i class="fa fa-check"></i><b>5.1.2</b> Not recommended workflow: Averaging the data</a></li>
<li class="chapter" data-level="5.1.3" data-path="workflow.html"><a href="workflow.html#sec:badworkflowb"><i class="fa fa-check"></i><b>5.1.3</b> Not recommended workflow: Stack imputed data</a></li>
<li class="chapter" data-level="5.1.4" data-path="workflow.html"><a href="workflow.html#sec:repeated"><i class="fa fa-check"></i><b>5.1.4</b> Repeated analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-pooling.html"><a href="sec-pooling.html"><i class="fa fa-check"></i><b>5.2</b> Parameter pooling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-pooling.html"><a href="sec-pooling.html#scalar-inference-of-normal-quantities"><i class="fa fa-check"></i><b>5.2.1</b> Scalar inference of normal quantities</a></li>
<li class="chapter" data-level="5.2.2" data-path="sec-pooling.html"><a href="sec-pooling.html#sec:poolnon"><i class="fa fa-check"></i><b>5.2.2</b> Scalar inference of non-normal quantities</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="sec-multiparameter.html"><a href="sec-multiparameter.html"><i class="fa fa-check"></i><b>5.3</b> Multi-parameter inference</a><ul>
<li class="chapter" data-level="5.3.1" data-path="sec-multiparameter.html"><a href="sec-multiparameter.html#sec:wald"><i class="fa fa-check"></i><b>5.3.1</b> <span class="math inline">\(D_1\)</span> Multivariate Wald test</a></li>
<li class="chapter" data-level="5.3.2" data-path="sec-multiparameter.html"><a href="sec-multiparameter.html#sec:chi"><i class="fa fa-check"></i><b>5.3.2</b> <span class="math inline">\(D_2\)</span> Combining test statistics<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="5.3.3" data-path="sec-multiparameter.html"><a href="sec-multiparameter.html#sec:likelihoodratio"><i class="fa fa-check"></i><b>5.3.3</b> <span class="math inline">\(D_3\)</span> Likelihood ratio test<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="5.3.4" data-path="sec-multiparameter.html"><a href="sec-multiparameter.html#d_1-d_2-or-d_3"><i class="fa fa-check"></i><b>5.3.4</b> <span class="math inline">\(D_1\)</span>, <span class="math inline">\(D_2\)</span> or <span class="math inline">\(D_3\)</span>?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="sec-stepwise.html"><a href="sec-stepwise.html"><i class="fa fa-check"></i><b>5.4</b> Stepwise model selection</a><ul>
<li class="chapter" data-level="5.4.1" data-path="sec-stepwise.html"><a href="sec-stepwise.html#variable-selection-techniques"><i class="fa fa-check"></i><b>5.4.1</b> Variable selection techniques</a></li>
<li class="chapter" data-level="5.4.2" data-path="sec-stepwise.html"><a href="sec-stepwise.html#computation"><i class="fa fa-check"></i><b>5.4.2</b> Computation</a></li>
<li class="chapter" data-level="5.4.3" data-path="sec-stepwise.html"><a href="sec-stepwise.html#sec:optimism"><i class="fa fa-check"></i><b>5.4.3</b> Model optimism</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="parallel-computation.html"><a href="parallel-computation.html"><i class="fa fa-check"></i><b>5.5</b> Parallel computation</a></li>
<li class="chapter" data-level="5.6" data-path="conclusion-4.html"><a href="conclusion-4.html"><i class="fa fa-check"></i><b>5.6</b> Conclusion</a></li>
<li class="chapter" data-level="5.7" data-path="ex-ch-analysis.html"><a href="ex-ch-analysis.html"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Part II: Advanced techniques</b></span></li>
<li class="chapter" data-level="6" data-path="ch-practice.html"><a href="ch-practice.html"><i class="fa fa-check"></i><b>6</b> Imputation in practice</a><ul>
<li class="chapter" data-level="6.1" data-path="sec-choices.html"><a href="sec-choices.html"><i class="fa fa-check"></i><b>6.1</b> Overview of modeling choices</a></li>
<li class="chapter" data-level="6.2" data-path="sec-whenignorable.html"><a href="sec-whenignorable.html"><i class="fa fa-check"></i><b>6.2</b> Ignorable or nonignorable?</a></li>
<li class="chapter" data-level="6.3" data-path="sec-modelform.html"><a href="sec-modelform.html"><i class="fa fa-check"></i><b>6.3</b> Model form and predictors</a><ul>
<li class="chapter" data-level="6.3.1" data-path="sec-modelform.html"><a href="sec-modelform.html#model-form"><i class="fa fa-check"></i><b>6.3.1</b> Model form</a></li>
<li class="chapter" data-level="6.3.2" data-path="sec-modelform.html"><a href="sec-modelform.html#sec:predictors"><i class="fa fa-check"></i><b>6.3.2</b> Predictors</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="sec-knowledge.html"><a href="sec-knowledge.html"><i class="fa fa-check"></i><b>6.4</b> Derived variables</a><ul>
<li class="chapter" data-level="6.4.1" data-path="sec-knowledge.html"><a href="sec-knowledge.html#sec:ratio"><i class="fa fa-check"></i><b>6.4.1</b> Ratio of two variables</a></li>
<li class="chapter" data-level="6.4.2" data-path="sec-knowledge.html"><a href="sec-knowledge.html#sec:interactions"><i class="fa fa-check"></i><b>6.4.2</b> Interaction terms</a></li>
<li class="chapter" data-level="6.4.3" data-path="sec-knowledge.html"><a href="sec-knowledge.html#sec:quadratic"><i class="fa fa-check"></i><b>6.4.3</b> Quadratic relations<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="6.4.4" data-path="sec-knowledge.html"><a href="sec-knowledge.html#compositional-dataspadesuit"><i class="fa fa-check"></i><b>6.4.4</b> Compositional data<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="6.4.5" data-path="sec-knowledge.html"><a href="sec-knowledge.html#sec:sumscores"><i class="fa fa-check"></i><b>6.4.5</b> Sum scores</a></li>
<li class="chapter" data-level="6.4.6" data-path="sec-knowledge.html"><a href="sec-knowledge.html#conditional-imputation"><i class="fa fa-check"></i><b>6.4.6</b> Conditional imputation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="sec-algoptions.html"><a href="sec-algoptions.html"><i class="fa fa-check"></i><b>6.5</b> Algorithmic options</a><ul>
<li class="chapter" data-level="6.5.1" data-path="sec-algoptions.html"><a href="sec-algoptions.html#sec:visit"><i class="fa fa-check"></i><b>6.5.1</b> Visit sequence</a></li>
<li class="chapter" data-level="6.5.2" data-path="sec-algoptions.html"><a href="sec-algoptions.html#sec:convergence"><i class="fa fa-check"></i><b>6.5.2</b> Convergence</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="sec-diagnostics.html"><a href="sec-diagnostics.html"><i class="fa fa-check"></i><b>6.6</b> Diagnostics</a><ul>
<li class="chapter" data-level="6.6.1" data-path="sec-diagnostics.html"><a href="sec-diagnostics.html#sec:fitversus"><i class="fa fa-check"></i><b>6.6.1</b> Model fit versus distributional discrepancy</a></li>
<li class="chapter" data-level="6.6.2" data-path="sec-diagnostics.html"><a href="sec-diagnostics.html#diagnostic-graphs"><i class="fa fa-check"></i><b>6.6.2</b> Diagnostic graphs</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="conclusion-5.html"><a href="conclusion-5.html"><i class="fa fa-check"></i><b>6.7</b> Conclusion</a></li>
<li class="chapter" data-level="6.8" data-path="ex-ch-practice.html"><a href="ex-ch-practice.html"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-multilevel.html"><a href="ch-multilevel.html"><i class="fa fa-check"></i><b>7</b> Multilevel multiple imputation</a><ul>
<li class="chapter" data-level="7.1" data-path="sec-multi-intro.html"><a href="sec-multi-intro.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="sec-threeformulations.html"><a href="sec-threeformulations.html"><i class="fa fa-check"></i><b>7.2</b> Notation for multilevel models</a></li>
<li class="chapter" data-level="7.3" data-path="sec-missmult.html"><a href="sec-missmult.html"><i class="fa fa-check"></i><b>7.3</b> Missing values in multilevel data</a><ul>
<li class="chapter" data-level="7.3.1" data-path="sec-missmult.html"><a href="sec-missmult.html#practical-issues-in-multilevel-imputation"><i class="fa fa-check"></i><b>7.3.1</b> Practical issues in multilevel imputation</a></li>
<li class="chapter" data-level="7.3.2" data-path="sec-missmult.html"><a href="sec-missmult.html#ad-hoc-solutions-for-multilevel-data"><i class="fa fa-check"></i><b>7.3.2</b> Ad-hoc solutions for multilevel data</a></li>
<li class="chapter" data-level="7.3.3" data-path="sec-missmult.html"><a href="sec-missmult.html#likelihood-solutions"><i class="fa fa-check"></i><b>7.3.3</b> Likelihood solutions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="sec-mljoint.html"><a href="sec-mljoint.html"><i class="fa fa-check"></i><b>7.4</b> Multilevel imputation by joint modeling</a></li>
<li class="chapter" data-level="7.5" data-path="sec-mlfcs.html"><a href="sec-mlfcs.html"><i class="fa fa-check"></i><b>7.5</b> Multilevel imputation by fully conditional specification</a><ul>
<li class="chapter" data-level="7.5.1" data-path="sec-mlfcs.html"><a href="sec-mlfcs.html#sec:clustermeans"><i class="fa fa-check"></i><b>7.5.1</b> Add cluster means of predictors</a></li>
<li class="chapter" data-level="7.5.2" data-path="sec-mlfcs.html"><a href="sec-mlfcs.html#sec:hetero"><i class="fa fa-check"></i><b>7.5.2</b> Model cluster heterogeneity</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="sec-multioutcome.html"><a href="sec-multioutcome.html"><i class="fa fa-check"></i><b>7.6</b> Continuous outcome</a><ul>
<li class="chapter" data-level="7.6.1" data-path="sec-multioutcome.html"><a href="sec-multioutcome.html#general-principle"><i class="fa fa-check"></i><b>7.6.1</b> General principle</a></li>
<li class="chapter" data-level="7.6.2" data-path="sec-multioutcome.html"><a href="sec-multioutcome.html#methods"><i class="fa fa-check"></i><b>7.6.2</b> Methods</a></li>
<li class="chapter" data-level="7.6.3" data-path="sec-multioutcome.html"><a href="sec-multioutcome.html#sec:contexam"><i class="fa fa-check"></i><b>7.6.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="sec-catoutcome.html"><a href="sec-catoutcome.html"><i class="fa fa-check"></i><b>7.7</b> Discrete outcome</a><ul>
<li class="chapter" data-level="7.7.1" data-path="sec-catoutcome.html"><a href="sec-catoutcome.html#methods-1"><i class="fa fa-check"></i><b>7.7.1</b> Methods</a></li>
<li class="chapter" data-level="7.7.2" data-path="sec-catoutcome.html"><a href="sec-catoutcome.html#example"><i class="fa fa-check"></i><b>7.7.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="sec-level2pred.html"><a href="sec-level2pred.html"><i class="fa fa-check"></i><b>7.8</b> Imputation of level-2 variable</a></li>
<li class="chapter" data-level="7.9" data-path="sec-comparative.html"><a href="sec-comparative.html"><i class="fa fa-check"></i><b>7.9</b> Comparative work</a></li>
<li class="chapter" data-level="7.10" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html"><i class="fa fa-check"></i><b>7.10</b> Guidelines and advice</a><ul>
<li class="chapter" data-level="7.10.1" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:emptymodel"><i class="fa fa-check"></i><b>7.10.1</b> Intercept-only model, missing outcomes</a></li>
<li class="chapter" data-level="7.10.2" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:ri1pred"><i class="fa fa-check"></i><b>7.10.2</b> Random intercepts, missing level-1 predictor</a></li>
<li class="chapter" data-level="7.10.3" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:wbg"><i class="fa fa-check"></i><b>7.10.3</b> Random intercepts, contextual model</a></li>
<li class="chapter" data-level="7.10.4" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:ril2"><i class="fa fa-check"></i><b>7.10.4</b> Random intercepts, missing level-2 predictor</a></li>
<li class="chapter" data-level="7.10.5" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:mlint"><i class="fa fa-check"></i><b>7.10.5</b> Random intercepts, interactions</a></li>
<li class="chapter" data-level="7.10.6" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:randomslopes"><i class="fa fa-check"></i><b>7.10.6</b> Random slopes, missing outcomes and predictors</a></li>
<li class="chapter" data-level="7.10.7" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:rsinteractions"><i class="fa fa-check"></i><b>7.10.7</b> Random slopes, interactions</a></li>
<li class="chapter" data-level="7.10.8" data-path="sec-mlguidelines.html"><a href="sec-mlguidelines.html#sec:recipes"><i class="fa fa-check"></i><b>7.10.8</b> Recipes</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="future-research.html"><a href="future-research.html"><i class="fa fa-check"></i><b>7.11</b> Future research</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-ice.html"><a href="ch-ice.html"><i class="fa fa-check"></i><b>8</b> Individual causal effects</a><ul>
<li class="chapter" data-level="8.1" data-path="sec-whyice.html"><a href="sec-whyice.html"><i class="fa fa-check"></i><b>8.1</b> Need for individual causal effects</a></li>
<li class="chapter" data-level="8.2" data-path="problem-of-causal-inference.html"><a href="problem-of-causal-inference.html"><i class="fa fa-check"></i><b>8.2</b> Problem of causal inference</a></li>
<li class="chapter" data-level="8.3" data-path="sec-iceframework.html"><a href="sec-iceframework.html"><i class="fa fa-check"></i><b>8.3</b> Framework</a></li>
<li class="chapter" data-level="8.4" data-path="generating-imputations-by-fcs.html"><a href="generating-imputations-by-fcs.html"><i class="fa fa-check"></i><b>8.4</b> Generating imputations by FCS</a><ul>
<li class="chapter" data-level="8.4.1" data-path="generating-imputations-by-fcs.html"><a href="generating-imputations-by-fcs.html#naive-fcs"><i class="fa fa-check"></i><b>8.4.1</b> Naive FCS</a></li>
<li class="chapter" data-level="8.4.2" data-path="generating-imputations-by-fcs.html"><a href="generating-imputations-by-fcs.html#sec:fcsprior"><i class="fa fa-check"></i><b>8.4.2</b> FCS with a prior for <span class="math inline">\(\rho\)</span></a></li>
<li class="chapter" data-level="8.4.3" data-path="generating-imputations-by-fcs.html"><a href="generating-imputations-by-fcs.html#sec:iceextensions"><i class="fa fa-check"></i><b>8.4.3</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="bibliographic-notes.html"><a href="bibliographic-notes.html"><i class="fa fa-check"></i><b>8.5</b> Bibliographic notes</a></li>
</ul></li>
<li class="part"><span><b>III Part III: Case studies</b></span></li>
<li class="chapter" data-level="9" data-path="ch-measurement.html"><a href="ch-measurement.html"><i class="fa fa-check"></i><b>9</b> Measurement issues</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-toomany.html"><a href="sec-toomany.html"><i class="fa fa-check"></i><b>9.1</b> Too many columns</a><ul>
<li class="chapter" data-level="9.1.1" data-path="sec-toomany.html"><a href="sec-toomany.html#sec:c85question"><i class="fa fa-check"></i><b>9.1.1</b> Scientific question</a></li>
<li class="chapter" data-level="9.1.2" data-path="sec-toomany.html"><a href="sec-toomany.html#sec:leiden85cohort"><i class="fa fa-check"></i><b>9.1.2</b> Leiden 85+ Cohort</a></li>
<li class="chapter" data-level="9.1.3" data-path="sec-toomany.html"><a href="sec-toomany.html#sec:exploration"><i class="fa fa-check"></i><b>9.1.3</b> Data exploration</a></li>
<li class="chapter" data-level="9.1.4" data-path="sec-toomany.html"><a href="sec-toomany.html#c85:influx"><i class="fa fa-check"></i><b>9.1.4</b> Outflux</a></li>
<li class="chapter" data-level="9.1.5" data-path="sec-toomany.html"><a href="sec-toomany.html#finding-problems-loggedevents"><i class="fa fa-check"></i><b>9.1.5</b> Finding problems: <code>loggedEvents</code></a></li>
<li class="chapter" data-level="9.1.6" data-path="sec-toomany.html"><a href="sec-toomany.html#quick-predictor-selection-quickpred"><i class="fa fa-check"></i><b>9.1.6</b> Quick predictor selection: <code>quickpred</code></a></li>
<li class="chapter" data-level="9.1.7" data-path="sec-toomany.html"><a href="sec-toomany.html#generating-the-imputations"><i class="fa fa-check"></i><b>9.1.7</b> Generating the imputations</a></li>
<li class="chapter" data-level="9.1.8" data-path="sec-toomany.html"><a href="sec-toomany.html#a-further-improvement-survival-as-predictor-variable"><i class="fa fa-check"></i><b>9.1.8</b> A further improvement: Survival as predictor variable</a></li>
<li class="chapter" data-level="9.1.9" data-path="sec-toomany.html"><a href="sec-toomany.html#some-guidance"><i class="fa fa-check"></i><b>9.1.9</b> Some guidance</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>9.2</b> Sensitivity analysis</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#sec:c85causes"><i class="fa fa-check"></i><b>9.2.1</b> Causes and consequences of missing data</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#scenarios"><i class="fa fa-check"></i><b>9.2.2</b> Scenarios</a></li>
<li class="chapter" data-level="9.2.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#generating-imputations-under-the-delta-adjustment"><i class="fa fa-check"></i><b>9.2.3</b> Generating imputations under the <span class="math inline">\(\delta\)</span>-adjustment</a></li>
<li class="chapter" data-level="9.2.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#complete-data-model"><i class="fa fa-check"></i><b>9.2.4</b> Complete-data model</a></li>
<li class="chapter" data-level="9.2.5" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#conclusion-6"><i class="fa fa-check"></i><b>9.2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-prevalence.html"><a href="sec-prevalence.html"><i class="fa fa-check"></i><b>9.3</b> Correct prevalence estimates from self-reported data</a><ul>
<li class="chapter" data-level="9.3.1" data-path="sec-prevalence.html"><a href="sec-prevalence.html#description-of-the-problem"><i class="fa fa-check"></i><b>9.3.1</b> Description of the problem</a></li>
<li class="chapter" data-level="9.3.2" data-path="sec-prevalence.html"><a href="sec-prevalence.html#sec:dontcount"><i class="fa fa-check"></i><b>9.3.2</b> Donâ€™t count on predictions</a></li>
<li class="chapter" data-level="9.3.3" data-path="sec-prevalence.html"><a href="sec-prevalence.html#the-main-idea"><i class="fa fa-check"></i><b>9.3.3</b> The main idea</a></li>
<li class="chapter" data-level="9.3.4" data-path="sec-prevalence.html"><a href="sec-prevalence.html#sec:srcdata"><i class="fa fa-check"></i><b>9.3.4</b> Data</a></li>
<li class="chapter" data-level="9.3.5" data-path="sec-prevalence.html"><a href="sec-prevalence.html#application"><i class="fa fa-check"></i><b>9.3.5</b> Application</a></li>
<li class="chapter" data-level="9.3.6" data-path="sec-prevalence.html"><a href="sec-prevalence.html#conclusion-7"><i class="fa fa-check"></i><b>9.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html"><i class="fa fa-check"></i><b>9.4</b> Enhancing comparability</a><ul>
<li class="chapter" data-level="9.4.1" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#description-of-the-problem-1"><i class="fa fa-check"></i><b>9.4.1</b> Description of the problem</a></li>
<li class="chapter" data-level="9.4.2" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#sec:equating"><i class="fa fa-check"></i><b>9.4.2</b> Full dependence: Simple equating</a></li>
<li class="chapter" data-level="9.4.3" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#sec:walkingimputation"><i class="fa fa-check"></i><b>9.4.3</b> Independence: Imputation without a bridge study</a></li>
<li class="chapter" data-level="9.4.4" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#sec:untenable"><i class="fa fa-check"></i><b>9.4.4</b> Fully dependent or independent?</a></li>
<li class="chapter" data-level="9.4.5" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#sec:impbridge"><i class="fa fa-check"></i><b>9.4.5</b> Imputation using a bridge study</a></li>
<li class="chapter" data-level="9.4.6" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#sec:walkinginterpretation"><i class="fa fa-check"></i><b>9.4.6</b> Interpretation</a></li>
<li class="chapter" data-level="9.4.7" data-path="sec-codingsystems.html"><a href="sec-codingsystems.html#conclusion-8"><i class="fa fa-check"></i><b>9.4.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ex-ch-measurement.html"><a href="ex-ch-measurement.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-selection.html"><a href="ch-selection.html"><i class="fa fa-check"></i><b>10</b> Selection issues</a><ul>
<li class="chapter" data-level="10.1" data-path="sec-selective.html"><a href="sec-selective.html"><i class="fa fa-check"></i><b>10.1</b> Correcting for selective drop-out</a><ul>
<li class="chapter" data-level="10.1.1" data-path="sec-selective.html"><a href="sec-selective.html#pops-study-19-years-follow-up"><i class="fa fa-check"></i><b>10.1.1</b> POPS study: 19 years follow-up</a></li>
<li class="chapter" data-level="10.1.2" data-path="sec-selective.html"><a href="sec-selective.html#characterization-of-the-drop-out"><i class="fa fa-check"></i><b>10.1.2</b> Characterization of the drop-out</a></li>
<li class="chapter" data-level="10.1.3" data-path="sec-selective.html"><a href="sec-selective.html#sec:popsmodel"><i class="fa fa-check"></i><b>10.1.3</b> Imputation model</a></li>
<li class="chapter" data-level="10.1.4" data-path="sec-selective.html"><a href="sec-selective.html#sec:degenerate"><i class="fa fa-check"></i><b>10.1.4</b> A solution â€œthat does not look goodâ€</a></li>
<li class="chapter" data-level="10.1.5" data-path="sec-selective.html"><a href="sec-selective.html#results"><i class="fa fa-check"></i><b>10.1.5</b> Results</a></li>
<li class="chapter" data-level="10.1.6" data-path="sec-selective.html"><a href="sec-selective.html#conclusion-9"><i class="fa fa-check"></i><b>10.1.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html"><i class="fa fa-check"></i><b>10.2</b> Correcting for nonresponse</a><ul>
<li class="chapter" data-level="10.2.1" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#fifth-dutch-growth-study"><i class="fa fa-check"></i><b>10.2.1</b> Fifth Dutch Growth Study</a></li>
<li class="chapter" data-level="10.2.2" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#nonresponse"><i class="fa fa-check"></i><b>10.2.2</b> Nonresponse</a></li>
<li class="chapter" data-level="10.2.3" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#comparison-to-known-population-totals"><i class="fa fa-check"></i><b>10.2.3</b> Comparison to known population totals</a></li>
<li class="chapter" data-level="10.2.4" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#sec:augmentsample"><i class="fa fa-check"></i><b>10.2.4</b> Augmenting the sample</a></li>
<li class="chapter" data-level="10.2.5" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#imputation-model"><i class="fa fa-check"></i><b>10.2.5</b> Imputation model</a></li>
<li class="chapter" data-level="10.2.6" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#sec:finalheight"><i class="fa fa-check"></i><b>10.2.6</b> Influence of nonresponse on final height</a></li>
<li class="chapter" data-level="10.2.7" data-path="sec-nonresponse.html"><a href="sec-nonresponse.html#discussion"><i class="fa fa-check"></i><b>10.2.7</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ex-ch-selection.html"><a href="ex-ch-selection.html"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ch-longitudinal.html"><a href="ch-longitudinal.html"><i class="fa fa-check"></i><b>11</b> Longitudinal data</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-longandwide.html"><a href="sec-longandwide.html"><i class="fa fa-check"></i><b>11.1</b> Long and wide format</a></li>
<li class="chapter" data-level="11.2" data-path="sec-fdd.html"><a href="sec-fdd.html"><i class="fa fa-check"></i><b>11.2</b> SE Fireworks Disaster Study</a><ul>
<li class="chapter" data-level="11.2.1" data-path="sec-fdd.html"><a href="sec-fdd.html#intention-to-treat"><i class="fa fa-check"></i><b>11.2.1</b> Intention to treat</a></li>
<li class="chapter" data-level="11.2.2" data-path="sec-fdd.html"><a href="sec-fdd.html#imputation-model-1"><i class="fa fa-check"></i><b>11.2.2</b> Imputation model</a></li>
<li class="chapter" data-level="11.2.3" data-path="sec-fdd.html"><a href="sec-fdd.html#inspecting-imputations"><i class="fa fa-check"></i><b>11.2.3</b> Inspecting imputations</a></li>
<li class="chapter" data-level="11.2.4" data-path="sec-fdd.html"><a href="sec-fdd.html#complete-data-model-1"><i class="fa fa-check"></i><b>11.2.4</b> Complete-data model</a></li>
<li class="chapter" data-level="11.2.5" data-path="sec-fdd.html"><a href="sec-fdd.html#results-from-the-complete-data-model"><i class="fa fa-check"></i><b>11.2.5</b> Results from the complete-data model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="sec-rastering.html"><a href="sec-rastering.html"><i class="fa fa-check"></i><b>11.3</b> Time raster imputation</a><ul>
<li class="chapter" data-level="11.3.1" data-path="sec-rastering.html"><a href="sec-rastering.html#change-score"><i class="fa fa-check"></i><b>11.3.1</b> Change score</a></li>
<li class="chapter" data-level="11.3.2" data-path="sec-rastering.html"><a href="sec-rastering.html#sec:tbcquestion"><i class="fa fa-check"></i><b>11.3.2</b> Scientific question: Critical periods</a></li>
<li class="chapter" data-level="11.3.3" data-path="sec-rastering.html"><a href="sec-rastering.html#sec:brokenstick"><i class="fa fa-check"></i><b>11.3.3</b> Broken stick model<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="11.3.4" data-path="sec-rastering.html"><a href="sec-rastering.html#terneuzen-birth-cohort"><i class="fa fa-check"></i><b>11.3.4</b> Terneuzen Birth Cohort</a></li>
<li class="chapter" data-level="11.3.5" data-path="sec-rastering.html"><a href="sec-rastering.html#sec:shrinkage"><i class="fa fa-check"></i><b>11.3.5</b> Shrinkage and the change score<span class="math inline">\(^\spadesuit\)</span></a></li>
<li class="chapter" data-level="11.3.6" data-path="sec-rastering.html"><a href="sec-rastering.html#sec:tbcimpute"><i class="fa fa-check"></i><b>11.3.6</b> Imputation</a></li>
<li class="chapter" data-level="11.3.7" data-path="sec-rastering.html"><a href="sec-rastering.html#complete-data-model-2"><i class="fa fa-check"></i><b>11.3.7</b> Complete-data model</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="conclusion-10.html"><a href="conclusion-10.html"><i class="fa fa-check"></i><b>11.4</b> Conclusion</a></li>
<li class="chapter" data-level="11.5" data-path="ex-ch-longitudinal.html"><a href="ex-ch-longitudinal.html"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Part IV: Extensions</b></span></li>
<li class="chapter" data-level="12" data-path="ch-conclusion.html"><a href="ch-conclusion.html"><i class="fa fa-check"></i><b>12</b> Conclusion</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-limitations.html"><a href="sec-limitations.html"><i class="fa fa-check"></i><b>12.1</b> Some dangers, some doâ€™s and some donâ€™ts</a><ul>
<li class="chapter" data-level="12.1.1" data-path="sec-limitations.html"><a href="sec-limitations.html#some-dangers"><i class="fa fa-check"></i><b>12.1.1</b> Some dangers</a></li>
<li class="chapter" data-level="12.1.2" data-path="sec-limitations.html"><a href="sec-limitations.html#sec:dos"><i class="fa fa-check"></i><b>12.1.2</b> Some doâ€™s</a></li>
<li class="chapter" data-level="12.1.3" data-path="sec-limitations.html"><a href="sec-limitations.html#sec:donts"><i class="fa fa-check"></i><b>12.1.3</b> Some donâ€™ts</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="sec-reporting.html"><a href="sec-reporting.html"><i class="fa fa-check"></i><b>12.2</b> Reporting</a><ul>
<li class="chapter" data-level="12.2.1" data-path="sec-reporting.html"><a href="sec-reporting.html#sec:guidelines"><i class="fa fa-check"></i><b>12.2.1</b> Reporting guidelines</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec-reporting.html"><a href="sec-reporting.html#sec:template"><i class="fa fa-check"></i><b>12.2.2</b> Template</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sec-otherapps.html"><a href="sec-otherapps.html"><i class="fa fa-check"></i><b>12.3</b> Other applications</a><ul>
<li class="chapter" data-level="12.3.1" data-path="sec-otherapps.html"><a href="sec-otherapps.html#synthetic-datasets-for-data-protection"><i class="fa fa-check"></i><b>12.3.1</b> Synthetic datasets for data protection</a></li>
<li class="chapter" data-level="12.3.2" data-path="sec-otherapps.html"><a href="sec-otherapps.html#analysis-of-coarsened-data"><i class="fa fa-check"></i><b>12.3.2</b> Analysis of coarsened data</a></li>
<li class="chapter" data-level="12.3.3" data-path="sec-otherapps.html"><a href="sec-otherapps.html#file-matching-of-multiple-datasets"><i class="fa fa-check"></i><b>12.3.3</b> File matching of multiple datasets</a></li>
<li class="chapter" data-level="12.3.4" data-path="sec-otherapps.html"><a href="sec-otherapps.html#planned-missing-data-for-efficient-designs"><i class="fa fa-check"></i><b>12.3.4</b> Planned missing data for efficient designs</a></li>
<li class="chapter" data-level="12.3.5" data-path="sec-otherapps.html"><a href="sec-otherapps.html#adjusting-for-verification-bias"><i class="fa fa-check"></i><b>12.3.5</b> Adjusting for verification bias</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="sec-future.html"><a href="sec-future.html"><i class="fa fa-check"></i><b>12.4</b> Future developments</a><ul>
<li class="chapter" data-level="12.4.1" data-path="sec-future.html"><a href="sec-future.html#derived-variables"><i class="fa fa-check"></i><b>12.4.1</b> Derived variables</a></li>
<li class="chapter" data-level="12.4.2" data-path="sec-future.html"><a href="sec-future.html#algorithms-for-blocks-and-batches"><i class="fa fa-check"></i><b>12.4.2</b> Algorithms for blocks and batches</a></li>
<li class="chapter" data-level="12.4.3" data-path="sec-future.html"><a href="sec-future.html#nested-imputation"><i class="fa fa-check"></i><b>12.4.3</b> Nested imputation</a></li>
<li class="chapter" data-level="12.4.4" data-path="sec-future.html"><a href="sec-future.html#better-trials-with-dynamic-treatment-regimes"><i class="fa fa-check"></i><b>12.4.4</b> Better trials with dynamic treatment regimes</a></li>
<li class="chapter" data-level="12.4.5" data-path="sec-future.html"><a href="sec-future.html#sec:free"><i class="fa fa-check"></i><b>12.4.5</b> Distribution-free pooling rules</a></li>
<li class="chapter" data-level="12.4.6" data-path="sec-future.html"><a href="sec-future.html#improved-diagnostic-techniques"><i class="fa fa-check"></i><b>12.4.6</b> Improved diagnostic techniques</a></li>
<li class="chapter" data-level="12.4.7" data-path="sec-future.html"><a href="sec-future.html#building-block-in-modular-statistics"><i class="fa fa-check"></i><b>12.4.7</b> Building block in modular statistics</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="ex-ch-conclusion.html"><a href="ex-ch-conclusion.html"><i class="fa fa-check"></i><b>12.5</b> Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="technical-information.html"><a href="technical-information.html"><i class="fa fa-check"></i><b>A</b> Technical information</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org/yihui/bookdown/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:FCS" class="section level2">
<h2><span class="header-section-number">4.5</span> Fully conditional specification</h2>
<div id="overview-3" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Overview</h3>
<p>Fully conditional specification (FCS) imputes multivariate missing data on a variable-by-variable basis <span class="citation">(Van Buuren et al. <a href="references.html#ref-VANBUUREN2006">2006</a>; Van Buuren <a href="references.html#ref-VANBUUREN2007">2007</a><a href="references.html#ref-VANBUUREN2007">a</a>)</span>. The method requires a specification of an imputation model for each incomplete variable, and creates imputations per variable in an iterative fashion.</p>
<p>In contrast to joint modeling, FCS specifies the multivariate distribution <span class="math inline">\(P(Y, X, R|\theta)\)</span> through a set of conditional densities <span class="math inline">\(P(Y_j | X, Y_{-j}, R, \phi_j)\)</span>. This conditional density is used to impute <span class="math inline">\(Y_j\)</span> given <span class="math inline">\(X\)</span>, <span class="math inline">\(Y_{-j}\)</span> and <span class="math inline">\(R\)</span>. Starting from simple random draws from the marginal distribution, imputation under FCS is done by iterating over the conditionally specified imputation models. The methods of Chapter <a href="ch-univariate.html#ch:univariate">3</a> may act as building blocks. FCS is a natural generalization of univariate imputation.</p>
<p><span class="citation">Rubin (<a href="references.html#ref-RUBIN1987">1987</a><a href="references.html#ref-RUBIN1987">b</a>, 160â€“66)</span> subdivided the work needed to create imputations into three tasks. The <em>modeling task</em> chooses a specific model for the data, the <em>estimation task</em> formulates the posterior parameters distribution given the model and the <em>imputation task</em> takes a random draws for the missing data by drawing successively from parameter and data distributions. FCS directly specifies the conditional distributions from which draws should be made, and hence bypasses the need to specify a multivariate model for the data.</p>
<p>The idea of conditionally specified models is quite old. Conditional probability distributions follow naturally from the theory of stochastic Markov chains <span class="citation">(Bartlett <a href="references.html#ref-BARTLETT1978">1978</a>, 34â€“41, pp.Â 231â€“236)</span>. In the context of spatial data, Besag preferred the use of conditional probability models over joint probability models, since â€œthe conditional probability approach has greater intuitive appeal to the practising statisticianâ€ <span class="citation">(Besag <a href="references.html#ref-BESAG1974">1974</a>, 223)</span>.</p>
<p>In the context of missing data imputation, similar ideas have surfaced under a variety of names: stochastic relaxation <span class="citation">(Kennickell <a href="references.html#ref-KENNICKELL1991">1991</a>)</span>, variable-by-variable imputation <span class="citation">(Brand <a href="references.html#ref-BRAND1999">1999</a>)</span>, switching regressions <span class="citation">(Van Buuren, Boshuizen, and Knook <a href="references.html#ref-VANBUUREN1999">1999</a>)</span>, sequential regressions <span class="citation">(Raghunathan et al. <a href="references.html#ref-RAGHUNATHAN2001">2001</a>)</span>, ordered pseudo-Gibbs sampler <span class="citation">(Heckerman et al. <a href="references.html#ref-HECKERMAN2001">2001</a>)</span>, partially incompatible MCMC <span class="citation">(Rubin <a href="references.html#ref-RUBIN2003">2003</a>)</span>, iterated univariate imputation <span class="citation">(Gelman <a href="references.html#ref-GELMAN2004">2004</a>)</span>, chained equations <span class="citation">(Van Buuren and Groothuis-Oudshoorn <a href="references.html#ref-VANBUUREN2000">2000</a>)</span> and fully conditional specification (FCS) <span class="citation">(Van Buuren et al. <a href="references.html#ref-VANBUUREN2006">2006</a>)</span>.</p>
</div>
<div id="sec:MICE" class="section level3">
<h3><span class="header-section-number">4.5.2</span> The MICE algorithm</h3>
<hr />

<div class="definition">
<p><span id="def:mice" class="definition"><strong>Algorithm 4.3  (MICE algorithm for imputation of multivariate missing data.<span class="math inline">\(^\spadesuit\)</span>)  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Specify an imputation model <span class="math inline">\(P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs}, Y_{-j}, R)\)</span> for variable <span class="math inline">\(Y_j\)</span> with <span class="math inline">\(j=1,\dots,p\)</span>.</p></li>
<li><p>For each <span class="math inline">\(j\)</span>, fill in starting imputations <span class="math inline">\(\dot Y_j^0\)</span> by random draws from <span class="math inline">\(Y_j^\mathrm{obs}\)</span>.</p></li>
<li><p>Repeat for <span class="math inline">\(t = 1,\dots,M\)</span>.</p></li>
<li><p>Repeat for <span class="math inline">\(j = 1,\dots,p\)</span>.</p></li>
<li><p>Define <span class="math inline">\(\dot Y_{-j}^t = (\dot Y_1^t,\dots,\dot Y_{j-1}^t,\dot Y_{j+1}^{t-1},\dots,\dot Y_p^{t-1})\)</span> as the currently complete data except <span class="math inline">\(Y_j\)</span>.</p></li>
<li><p>Draw <span class="math inline">\(\dot\phi_j^t \sim P(\phi_j^t|Y_j^\mathrm{obs}, \dot Y_{-j}^t, R)\)</span>.</p></li>
<li><p>Draw imputations <span class="math inline">\(\dot Y_j^t \sim P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs},  \dot Y_{-j}^t, R, \dot\phi_j^t)\)</span>.</p></li>
<li><p>End repeat <span class="math inline">\(j\)</span>.</p></li>
<li>End repeat <span class="math inline">\(t\)</span>.
</div>
</li>
</ol>

<hr />
<p>There are several ways to implement imputation under conditionally specified models. Algorithm <a href="sec-FCS.html#def:mice">4.3</a> describes one particular instance: the MICE algorithm <span class="citation">(Van Buuren and Groothuis-Oudshoorn <a href="references.html#ref-VANBUUREN2000">2000</a>; Van Buuren and Groothuis-Oudshoorn <a href="references.html#ref-VANBUUREN2011B">2011</a>)</span>. The algorithm starts with a random draw from the observed data, and imputes the incomplete data in a variable-by-variable fashion. One iteration consists of one cycle through all <span class="math inline">\(Y_j\)</span>. The number of iterations <span class="math inline">\(M\)</span> can often be low, say 5 or 10. The MICE algorithm generates multiple imputations by executing Algorithm <a href="sec-FCS.html#def:mice">4.3</a> in parallel <span class="math inline">\(m\)</span> times.</p>
<p>The MICE algorithm is a Markov chain Monte Carlo (MCMC) method, where the state space is the collection of all imputed values. More specifically, if the conditionals are compatible (cf.Â Section <a href="sec-FCS.html#sec:compatibility">4.5.3</a>), the MICE algorithm is a Gibbs sampler, a Bayesian simulation technique that samples from the conditional distributions in order to obtain samples from the joint distribution <span class="citation">(Gelfand and Smith <a href="references.html#ref-GELFAND1990">1990</a>; Casella and George <a href="references.html#ref-CASELLA1992">1992</a>)</span>. In conventional applications of the Gibbs sampler, the full conditional distributions are derived from the joint probability distribution <span class="citation">(Gilks <a href="references.html#ref-GILKS1996B">1996</a>)</span>. In the MICE algorithm, the conditional distributions are under direct control of the user, and so the joint distribution is only implicitly known, and may not actually exist. While the latter is clearly undesirable from a theoretical point of view (since we do not know the joint distribution to which the algorithm converges), in practice it does not seem to hinder useful applications of the method (cf.Â Section <a href="sec-FCS.html#sec:compatibility">4.5.3</a>).</p>
<p>In order to converge to a stationary distribution, a Markov chain needs to satisfy three important properties <span class="citation">(Roberts <a href="references.html#ref-ROBERTS1996">1996</a>; Tierney <a href="references.html#ref-TIERNEY1996">1996</a>)</span>:</p>
<ul>
<li><p><em>irreducible</em>, the chain must be able to reach all interesting parts of the state space;</p></li>
<li><p><em>aperiodic</em>, the chain should not oscillate between different states;</p></li>
<li><p><em>recurrence</em>, all interesting parts can be reached infinitely often, at least from almost all starting points.</p></li>
</ul>
<p>Do these properties hold for the MICE algorithm? Irreducibility is generally not a problem since the user has large control over the interesting parts of the state space. This flexibility is actually the main rationale for FCS instead of a joint model.</p>
<p>Periodicity is a potential problem, and can arise in the situation where imputation models are clearly inconsistent. A rather artificial example of an oscillatory behavior occurs when <span class="math inline">\(Y_1\)</span> is imputed by <span class="math inline">\(Y_2\beta+\epsilon_1\)</span> and <span class="math inline">\(Y_2\)</span> is imputed by <span class="math inline">\(-Y_1\beta+\epsilon_2\)</span> for some fixed, nonzero <span class="math inline">\(\beta\)</span>. The sampler will oscillate between two qualitatively different states, so the correlation between <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> after imputing <span class="math inline">\(Y_1\)</span> will differ from that after imputing <span class="math inline">\(Y_2\)</span>. In general, we would like the statistical inferences to be independent of the stopping point. A way to diagnose the <em>ping-pong</em> problem, or <em>order effect</em>, is to stop the chain at different points. The stopping point should not affect the statistical inferences. The addition of noise to create imputations is a safeguard against periodicity, and allows the sampler to â€œbreak outâ€ more easily.</p>
<p>Non-recurrence may also be a potential difficulty, manifesting itself as explosive or non-stationary behavior. For example, if imputations are made by deterministic functions, the Markov chain may lock up. Such cases can sometimes be diagnosed from the trace lines of the sampler. See Section <a href="sec-algoptions.html#sec:convergence">6.5.2</a> for an example. As long as the parameters of imputation models are estimated from the data, non-recurrence is mild or absent.</p>
<p>The required properties of the MCMC method can be translated into conditions on the eigenvalues of the matrix of transition probabilities <span class="citation">(MacKay <a href="references.html#ref-MACKAY2003">2003</a>, 372â€“73)</span>. The development of practical tools that put these conditions to work for multiple imputation is still an ongoing research problem.</p>
</div>
<div id="sec:compatibility" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Compatibility<span class="math inline">\(^\spadesuit\)</span></h3>
<p>Gibbs sampling is based on the idea that knowledge of the conditional distributions is sufficient to determine a joint distribution, if it exists. Two conditional densities <span class="math inline">\(p(Y_1|Y_2)\)</span> and <span class="math inline">\(p(Y_2|Y_1)\)</span> are said to be <em>compatible</em> if a joint distribution <span class="math inline">\(p(Y_1,Y_2)\)</span> exists that has <span class="math inline">\(p(Y_1|Y_2)\)</span> and <span class="math inline">\(p(Y_2|Y_1)\)</span> as its conditional densities. More precisely, the two conditional densities are compatible if and only if their density ratio <span class="math inline">\(p(Y_1|Y_2)/p(Y_2|Y_1)\)</span> factorizes into the product <span class="math inline">\(u(Y_1)v(Y_2)\)</span> for some integrable functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> <span class="citation">(Besag <a href="references.html#ref-BESAG1974">1974</a>)</span>. So, the joint distribution either exists and is unique, or does not exist.</p>
<p>If the joint density itself is of genuine scientific interest, we should carefully evaluate the effect that imputations might have on the estimate of the distribution. For example, incompatible conditionals could produce a ridge (or spike) in an otherwise smooth density, and the location of the ridge may actually depend on the stopping point. If such is the case, then we should have a reason to favor a particular stopping point. Alternatively, we might try to reformulate the imputation model so that the order effect disappears.</p>
<p><span class="citation">Arnold and Press (<a href="references.html#ref-ARNOLD1989">1989</a>)</span> and <span class="citation">Arnold, Castillo, and Sarabia (<a href="references.html#ref-ARNOLD1999">1999</a>)</span> provide necessary and sufficient conditions for the existence of a joint distribution given two conditional densities. <span class="citation">Gelman and Speed (<a href="references.html#ref-GELMAN1993">1993</a>)</span> concentrate on the question whether an arbitrary mix of conditional and marginal distribution yields a unique joint distribution. <span class="citation">Arnold, Castillo, and Sarabia (<a href="references.html#ref-ARNOLD2002">2002</a>)</span> describe near-compatibility in discrete data.</p>
<p>Several papers are now available on the conditions under which imputations created by conditionally specified models are draws from the implicit joint distribution. According to <span class="citation">Hughes et al. (<a href="references.html#ref-HUGHES2014">2014</a>)</span>, two conditions must hold for this to occur. First, the conditionals must be compatible, and second the margin must be noninformative. Suppose that <span class="math inline">\(p(\phi_j)\)</span> is the prior distribution of the set of parameters that relate <span class="math inline">\(Y_j\)</span> to <span class="math inline">\(Y_{-j}\)</span>, and that <span class="math inline">\(p(\tilde\phi_j)\)</span> is prior distribution of the set of parameters that describes that relations among the <span class="math inline">\(Y_{-j}\)</span>. The noninformative margins condition states that if two sets of parameters are distinct (i.e., their joint parameter space is the product of their separate parameter spaces), and their joint distribution <span class="math inline">\(p(\phi_j, \tilde\phi_j)\)</span> are independent and factorizes as <span class="math inline">\(p(\phi_j, \tilde\phi_j) = p(\phi_j)p(\tilde\phi_j)\)</span>. Independence is a property of the prior distributions, whereas distinctness is a property of the model. <span class="citation">Hughes et al. (<a href="references.html#ref-HUGHES2014">2014</a>)</span> show that distinctness holds for the saturated multinomial distribution with a Dirichlet prior, so imputations from this joint distribution can be achieved by a set of conditionally specified models. However, for the log-linear model with only two-way factor interaction (and no higher-order terms) distinctness only holds for a maximum of three variables. The noninformative marginal condition is sufficient, but not necessary. In most practical cases we are unable to show that the noninformative marginal condition holds, but we can stop the algorithms at different points and inspect the estimates for order effect. Simulations by <span class="citation">Hughes et al. (<a href="references.html#ref-HUGHES2014">2014</a>)</span> show that such order effects exist, but in general they are small. <span class="citation">Liu et al. (<a href="references.html#ref-LIU2013">2013</a>)</span> made the same division in the parameter space of compatible models, and showed that imputation created by conditional specification is asymptotically equivalent to full Bayesian imputation for an assumed joint model. Asymptotic equivelance assumes infinite <span class="math inline">\(m\)</span> and infinite <span class="math inline">\(n\)</span>, and holds when the joint model is misspecified. The order effect disappear with increasing sample size. <span class="citation">Zhu and Raghunathan (<a href="references.html#ref-ZHU2015">2015</a>)</span> observed that the parameters of the conditionally specified models typically span a larger space than the space occupied by the implied joint model. A set of imputation models is <em>possibly compatible</em> if the conditional density for each variable <span class="math inline">\(j\)</span> according to some joint distribution is a special case of the corresponding imputation model for <span class="math inline">\(j\)</span>. If the parameters of the joint model can be separated, then iteration over the possible compatible conditional imputation models will provide draws from the conditional densities of the implied joint distribution.</p>
<p>Several methods for identifying compatibility from actual data have been developed <span class="citation">(Tian et al. <a href="references.html#ref-TIAN2009">2009</a>; Ip and Wang <a href="references.html#ref-IP2009">2009</a>; Wang and Kuo <a href="references.html#ref-WANG2010A">2010</a>; Chen <a href="references.html#ref-CHEN2011B">2011</a>; Yao, Chen, and Wang <a href="references.html#ref-YAO2014">2014</a>; Kuo, Song, and Jiang <a href="references.html#ref-KUO2017">2017</a>)</span>. However, the application of these methods is challenging because of the many possible choices of conditional models. What happens when the joint distribution does not exist? The MICE algorithm is ignorant of the non-existence of the joint distribution, and happily produces imputations whether the joint distribution exists or not. Can the imputed data be trusted when we cannot find a joint distribution <span class="math inline">\(p(Y_1,Y_2)\)</span> that has <span class="math inline">\(p(Y_1|Y_2)\)</span> and <span class="math inline">\(p(Y_2|Y_1)\)</span> as its conditionals?</p>
<p>Incompatibility easily arises if deterministic functions of the data are imputed along with their originals. For example, the imputation model may contain interaction terms, data summaries or nonlinear functions of the data. Such terms introduce feedback loops and impossible combinations into the system, which can invalidate the imputations <span class="citation">(Van Buuren and Groothuis-Oudshoorn <a href="references.html#ref-VANBUUREN2011B">2011</a>)</span>. It is important to diagnose this behavior and eliminate feedback loops from the system. Chapter <a href="ch-practice.html#ch:practice">6</a> describes the tools to do this.</p>
<p><span class="citation">Van Buuren et al. (<a href="references.html#ref-VANBUUREN2006">2006</a>)</span> described a small simulation study using strongly incompatible models. The adverse effects on the estimates after multiple imputation were only minimal in the cases studied. Though FCS is only guaranteed to work if the conditionals are compatible, these simulations suggested that the results may be robust against violations of compatibility. <span class="citation">Li, Yu, and Rubin (<a href="references.html#ref-LI2012">2012</a>)</span> presented three examples of problems with MICE. However, their examples differ from the usual sequential regression setup in various ways, and do not undermine the validity of the approach <span class="citation">(Zhu and Raghunathan <a href="references.html#ref-ZHU2015">2015</a>)</span>. <span class="citation">Liu et al. (<a href="references.html#ref-LIU2013">2013</a>)</span> pointed out that application of incompatible conditional models cannot provide imputations from any joint model. However, they also found that Rubinâ€™s rules provide consistent point estimates for incompatible models under fairly general conditions, as long as each conditional model was correctly specified. <span class="citation">Zhu and Raghunathan (<a href="references.html#ref-ZHU2015">2015</a>)</span> showed that incompatibility does not need to lead to divergence. While there is no joint model to converge to, the algorithm can still converge. The key in achieving convergence is that the imputation models should closely model the data. For example, include the skewness of the residuals, or ideally, generate the imputations from the underlying (but usually unknown) mechanism that generated the data.</p>
<p>The interesting point is that the last two papers have shifted the perspective from the userâ€™s joint model to the data producerâ€™s data generating model. With incompatible models, the most important condition is the validity of each conditional model. As long as the conditional models are able to replay the missing data according to the mechanism that generated the data, we might not be overly concerned with issues of compatibility.</p>
<p>In the majority of cases, scientific interest will focus on quantities that are more remote to the joint density, such as regression weights, factor loadings, prevalence estimates and so on. In such cases, the joint distribution is more like a nuisance factor that has no intrinsic value.</p>
<p>Apart from potential feedback problems, it appears that incompatibility seems like a relatively minor problem in practice, especially if the missing data rate is modest and the imputation models fit the data well. In order to evaluate these aspects, we need to inspect convergence and assess the fit of the imputations.</p>
</div>
<div id="sec:congeniality" class="section level3">
<h3><span class="header-section-number">4.5.4</span> Congeniality or compatibility?</h3>
<p><span class="citation">Meng (<a href="references.html#ref-MENG1994">1994</a>)</span> introduced the concept of <em>congeniality</em> to refer to the relation between the imputation model and the analysis model. Some recent papers have used the term <em>compatibility</em> to refer to essentially the same concept, and this alternative use of the term compatibility may generate confusion. This section explains the two different meanings attached to the compatibility.</p>
<p>Compatibility refers to the property that the conditionally specified models together specify some joint distribution from which imputations are to be drawn. Compatibility is a theoretical requirement of the Gibbs sampler. The evidence obtained thus far indicated that mutual incompatibility of conditionals will only have a minor impact on the final inferences, as long as the conditional models are well specified to fit the data. See Section <a href="sec-FCS.html#sec:compatibility">4.5.3</a> for more detail.</p>
<p>Another use of compatibility refers to the relation between the substantive model and the imputation model. It is widely accepted that the imputation model should be more general than the substantive model. <span class="citation">Meng (<a href="references.html#ref-MENG1994">1994</a>)</span> stated that the analysis procedure should be congenial to the imputation model, where congeniality is a property of the analysis procedure. <span class="citation">Bartlett et al. (<a href="references.html#ref-BARTLETT2015">2015</a>)</span> connected congeniality to compatibility by extending the joint distribution of the imputation model to include the substantive model. <span class="citation">Bartlett et al. (<a href="references.html#ref-BARTLETT2015">2015</a>)</span> reasoned that an imputation model is congenial to the substantive model if the two models are compatible. In that case, a joint model exists whose conditionals include both the imputation and substantive model. Models that are incompatible may lead to biased estimates of parameters in the substantive model. Hence, incompatibility is a bad thing that should be prevented. Technically speaking, the use of the term compatibility is correct, but the interpretation and implications of this form of incompatibility are very different, and in fact close in spirit to Mengâ€™s congeniality.</p>
<p>This book reserves the term <em>compatibility</em> to refer to the property of the imputation model whether its parts make up a joint distribution (irrespective of an analysis model), and use the term <em>congeniality</em> to refer to the relation between the imputation model and the substantive model.</p>
</div>
<div id="sec:modelbased" class="section level3">
<h3><span class="header-section-number">4.5.5</span> Model-based and data-based imputation</h3>
<p>This section highlights an interesting new development for setting up imputation models.</p>
<p>Observe that <span class="citation">Bartlett et al. (<a href="references.html#ref-BARTLETT2015">2015</a>)</span> reversed the direction of of the relation between the imputation and substantive models. <span class="citation">Meng (<a href="references.html#ref-MENG1994">1994</a>)</span> takes a given imputation model, and then asks whether the analysis model is congenial to it, whereas <span class="citation">Bartlett et al. (<a href="references.html#ref-BARTLETT2015">2015</a>)</span> start from the complete-data model, and ask whether the imputation model is congenial/compatible. These are complementary perspectives, leading to different strategies in setting up imputation models. If there is a strong scientific model, then it is natural to use model-based imputation, which puts the substantive model in the driverâ€™s seat, and ensures that the distribution from which imputations are generated is compatible to the substantive model. The challenge here is to create imputations that remain faithful to the data, and do not amplify aspects assumed in the model that are not supported by the data. If there is weak scientific theory, or if a wide range of models is fitted, then apply data-based imputation, where imputations are generated that closely represent the features of the data, without any particular analysis model in mind. Then the challenge is to create imputations that will accommodate for a wide range of substantive models.</p>
<p>The model-based approach is theoretically well grounded, and procedures are available for substantive models based on normal regression, discrete outcomes and proportional hazards. Related work can be found in <span class="citation">Wu (<a href="references.html#ref-WU2010">2010</a>)</span>, <span class="citation">Goldstein, Carpenter, and Browne (<a href="references.html#ref-GOLDSTEIN2014">2014</a>)</span>, <span class="citation">Erler et al. (<a href="references.html#ref-ERLER2016">2016</a>)</span>, <span class="citation">Erler et al. (<a href="references.html#ref-ERLER2018">2018</a>)</span> and <span class="citation">Zhang and Wang (<a href="references.html#ref-ZHANG2017">2017</a>)</span>. Some simulations showed promising results <span class="citation">(Grund, LÃ¼dtke, and Robitzsch <a href="references.html#ref-GRUND2018">2018</a><a href="references.html#ref-GRUND2018">b</a>)</span>. Implementations in <code>R</code> are available as the <code>smcfcs</code> package <span class="citation">(Bartlett and Keogh <a href="references.html#ref-BARTLETT2018">2018</a>)</span>, and the <code>mdmb</code> package <span class="citation">(Robitzsch and LÃ¼dtke <a href="references.html#ref-ROBITZSCH2018">2018</a>)</span>, as well as <code>Blimp</code> <span class="citation">(Enders, Keller, and Levy <a href="references.html#ref-ENDERS2018">2018</a>)</span>. One potential drawback is that the imputations might be specific to the model at hand, and need to be redone if the model changes. Another potential issue could be the calculations needed, which requires the use of rejection samplers. There is not yet much experience with such practicalities, but a method that approaches the imputation problem â€œfrom the other sideâ€ is an interesting and potentially useful addition to the imputerâ€™s toolbox.</p>
</div>
<div id="sec:howlarget" class="section level3">
<h3><span class="header-section-number">4.5.6</span> Number of iterations</h3>
<p>When <span class="math inline">\(m\)</span> sampling streams are calculated in parallel, monitoring convergence is done by plotting one or more statistics of interest in each stream against iteration number <span class="math inline">\(t\)</span>. Common statistics to be plotted are the mean and standard deviation of the synthetic data, as well as the correlation between different variables. The pattern should be free of trend, and the variance within a chain should approximate the variance between chains.</p>
<p>In practice, a low number of iterations appears to be enough. <span class="citation">Brand (<a href="references.html#ref-BRAND1999">1999</a>)</span> and <span class="citation">Van Buuren, Boshuizen, and Knook (<a href="references.html#ref-VANBUUREN1999">1999</a>)</span> set the number of iterations <span class="math inline">\(M\)</span> quite low, usually somewhere between 5 to 20 iterations. This number is much lower than in other applications of MCMC methods, which often require thousands of iterations.</p>
<p>Why can the number of iterations in MICE be so low? First of all, realize that the imputed data <span class="math inline">\(\dot Y_\mathrm{mis}\)</span> form the only memory in the the MICE algorithm. Chapter <a href="ch-univariate.html#ch:univariate">3</a> explained that imputed data can have a considerable amount of random noise, depending on the strength of the relations between the variables. Applications of MICE with lowly correlated data therefore inject a lot of noise into the system. Hence, the autocorrelation over <span class="math inline">\(t\)</span> will be low, and convergence will be rapid, and in fact immediate if all variables are independent. Thus, the incorporation of noise into the imputed data has pleasant side-effect of speeding up convergence. Reversely, situations to watch out for occur if:</p>
<ul>
<li><p>the correlations between the <span class="math inline">\(Y_j\)</span>â€™s are high;</p></li>
<li><p>the missing data rates are high; or</p></li>
<li><p>constraints on parameters across different variables exist.</p></li>
</ul>
<p>The first two conditions directly affect the amount of autocorrelation in the system. The latter condition becomes relevant for customized imputation models. We will see some examples in Section <a href="sec-algoptions.html#sec:convergence">6.5.2</a>.</p>
<p>In the context of missing data imputation, our simulations have shown that unbiased estimates and appropriate coverage usually requires no more than just five iterations. It is, however, important not to rely automatically on this result as some applications can require considerably more iterations.</p>
</div>
<div id="sec:slowconvergence" class="section level3">
<h3><span class="header-section-number">4.5.7</span> Example of slow convergence</h3>
<p>Consider a small simulation experiment with three variables: one complete covariate <span class="math inline">\(X\)</span> and two incomplete variables <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>. The data consist of draws from the multivariate normal distribution with correlations <span class="math inline">\(\rho(X,Y_1)= \rho(X,Y_2)=0.9\)</span> and <span class="math inline">\(\rho(Y_1,Y_2) = 0.7\)</span>. The variables are ordered as <span class="math inline">\([X, Y_1, Y_2]\)</span>. The complete pattern is <span class="math inline">\(R_1=(1, 1, 1)\)</span>. Missing data are randomly created in two patterns: <span class="math inline">\(R_2=(1, 0, 1)\)</span> and <span class="math inline">\(R_3=(1, 1, 0)\)</span>. Variables <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are jointly observed on <span class="math inline">\(n_{(1,1,1)}\)</span> complete cases. The following code defines the function to generate the incomplete data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">generate &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> <span class="kw">c</span>(<span class="dv">1000</span>, <span class="dv">4500</span>, <span class="dv">4500</span>, <span class="dv">0</span>),
                     <span class="dt">cor =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.9</span>, <span class="fl">0.9</span>,
                                    <span class="fl">0.9</span>, <span class="fl">1.0</span>, <span class="fl">0.7</span>,
                                    <span class="fl">0.9</span>, <span class="fl">0.7</span>, <span class="fl">1.0</span>), <span class="dt">nrow =</span> <span class="dv">3</span>)) {
  <span class="kw">require</span>(MASS)
  nt &lt;-<span class="st"> </span><span class="kw">sum</span>(n)
  cs &lt;-<span class="st"> </span><span class="kw">cumsum</span>(n)
  data &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(nt, <span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">3</span>), <span class="dt">Sigma =</span> cor)
  <span class="kw">dimnames</span>(data) &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dv">1</span><span class="op">:</span>nt, <span class="kw">c</span>(<span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y1&quot;</span>, <span class="st">&quot;Y2&quot;</span>))
  <span class="cf">if</span> (n[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) data[(cs[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>cs[<span class="dv">2</span>],<span class="st">&quot;Y1&quot;</span>] &lt;-<span class="st"> </span><span class="ot">NA</span>
  <span class="cf">if</span> (n[<span class="dv">3</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) data[(cs[<span class="dv">2</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>cs[<span class="dv">3</span>],<span class="st">&quot;Y2&quot;</span>] &lt;-<span class="st"> </span><span class="ot">NA</span>
  <span class="cf">if</span> (n[<span class="dv">4</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) data[(cs[<span class="dv">3</span>]<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>cs[<span class="dv">4</span>],<span class="kw">c</span>(<span class="st">&quot;Y1&quot;</span>,<span class="st">&quot;Y2&quot;</span>)] &lt;-<span class="st"> </span><span class="ot">NA</span>
  <span class="kw">return</span>(data)
}</code></pre></div>
<p>As an imputation model, we specified compatible linear regressions <span class="math inline">\(Y_1= \beta_{1,0} + \beta_{1,2}Y_2 + \beta_{1,3}X + \epsilon_1\)</span> and <span class="math inline">\(Y_2= \beta_{2,0} + \beta_{2,1}Y_1 + \beta_{2,3}X + \epsilon_2\)</span> to impute <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>. The following code defines the function used for imputation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">impute &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">m =</span> <span class="dv">5</span>, <span class="dt">method =</span> <span class="st">&quot;norm&quot;</span>,
                   <span class="dt">print =</span> <span class="ot">FALSE</span>, <span class="dt">maxit =</span> <span class="dv">10</span>, ...) {
  statistic &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow =</span> maxit, <span class="dt">ncol =</span> m)
  <span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>maxit) {
    <span class="cf">if</span> (iter<span class="op">==</span><span class="dv">1</span>) imp &lt;-<span class="st"> </span><span class="kw">mice</span>(data, <span class="dt">m =</span> m, <span class="dt">method =</span> method,
                             <span class="dt">print =</span> print, <span class="dt">maxit =</span> <span class="dv">1</span>, ...)
    <span class="cf">else</span> imp &lt;-<span class="st"> </span><span class="kw">mice.mids</span>(imp, <span class="dt">maxit =</span> <span class="dv">1</span>, <span class="dt">print =</span> print, ...)
    statistic[iter, ] &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">with</span>(imp, <span class="kw">cor</span>(Y1, Y2))<span class="op">$</span>analyses)
  }
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">imp =</span> imp, <span class="dt">statistic =</span> statistic))
}</code></pre></div>
<p>The difficulty in this particular problem is that the correlation <span class="math inline">\(\rho(Y_1,Y_2)\)</span> under the conditional independence of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> given <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(0.9 \times 0.9 = 0.81\)</span>, whereas the true value equals <span class="math inline">\(0.7\)</span>. It is thus of interest to study how the correlation <span class="math inline">\(\rho(Y_1,Y_2)\)</span> develops over the iterations, but this is not a standard function in <code>mice()</code>. As an alternative, the <code>impute()</code> function repeatedly calls <code>mice.mids()</code> with <code>maxit = 1</code>, and calculates <span class="math inline">\(\rho(Y_1, Y_2)\)</span> after each iteration from the complete data.</p>
<p>The following code defines six scenarios where the number of complete cases is varied as <span class="math inline">\(n_{(1,1,1)} \in \{1000, 500, 250, 100, 50, 0\}\)</span>, while holding the total sample size constant at <span class="math inline">\(n = 10000\)</span>. The proportion of complete rows thus varies between 10% and 0%.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">simulate &lt;-<span class="st"> </span><span class="cf">function</span>(
  <span class="dt">ns =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1000</span>, <span class="dv">500</span>, <span class="dv">250</span>, <span class="dv">100</span>, <span class="dv">50</span>, <span class="dv">0</span>,
                <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">4500</span>, <span class="dv">4750</span>, <span class="dv">4875</span>, <span class="dv">4950</span>, <span class="dv">4975</span>, <span class="dv">5000</span>), <span class="dv">2</span>),
                <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>)), <span class="dt">nrow =</span> <span class="dv">6</span>),
  <span class="dt">m =</span> <span class="dv">5</span>, <span class="dt">maxit =</span> <span class="dv">10</span>, <span class="dt">seed =</span> <span class="dv">1</span>, ...) {
  <span class="cf">if</span> (<span class="op">!</span><span class="kw">missing</span>(seed)) <span class="kw">set.seed</span>(seed)
  s &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(ns), <span class="dt">each =</span> maxit <span class="op">*</span><span class="st"> </span>m),
             <span class="kw">apply</span>(ns, <span class="dv">2</span>, rep, <span class="dt">each =</span> maxit <span class="op">*</span><span class="st"> </span>m),
             <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>maxit, <span class="dt">each =</span> m), <span class="dv">1</span><span class="op">:</span>m, <span class="ot">NA</span>)
  <span class="kw">colnames</span>(s) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;k&quot;</span>, <span class="st">&quot;n111&quot;</span>, <span class="st">&quot;n101&quot;</span>, <span class="st">&quot;n110&quot;</span>, <span class="st">&quot;n100&quot;</span>,
                   <span class="st">&quot;iteration&quot;</span>, <span class="st">&quot;m&quot;</span>, <span class="st">&quot;rY1Y2&quot;</span>)
  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(ns)) {
    data &lt;-<span class="st"> </span><span class="kw">generate</span>(ns[k, ], ...)
    r &lt;-<span class="st"> </span><span class="kw">impute</span>(data, <span class="dt">m =</span> m, <span class="dt">maxit =</span> maxit, ...)
    s[s[,<span class="st">&quot;k&quot;</span>] <span class="op">==</span><span class="st"> </span>k, <span class="st">&quot;rY1Y2&quot;</span>] &lt;-<span class="st"> </span><span class="kw">t</span>(r<span class="op">$</span>statistic)
  }
  <span class="kw">return</span>(<span class="kw">data.frame</span>(s))
}</code></pre></div>
<p>The <code>simulate()</code> function code collects the correlations <span class="math inline">\(\rho(Y_1,Y_2)\)</span> per iteration in the data frame <code>s</code>. Now call the function with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slow.demo &lt;-<span class="st"> </span><span class="kw">simulate</span>(<span class="dt">maxit =</span> <span class="dv">150</span>, <span class="dt">seed =</span> <span class="dv">62771</span>)</code></pre></div>
<div class="figure"><span id="fig:slowplot"></span>
<img src="fig/ch04-slowplot-1.png" alt="Correlation between \(Y_1\) and \(Y_2\) in the imputed data per iteration in five independent runs of the MICE algorithm for six levels of missing data. The true value is 0.7. The figure illustrates that convergence can be slow for high percentages of missing data." width="672" />
<p class="caption">
Figure 4.3: Correlation between <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> in the imputed data per iteration in five independent runs of the MICE algorithm for six levels of missing data. The true value is 0.7. The figure illustrates that convergence can be slow for high percentages of missing data.
</p>
</div>

<p>Figure <a href="sec-FCS.html#fig:slowplot">4.3</a> shows the development of <span class="math inline">\(\rho(Y_1,Y_2)\)</span> calculated on the completed data after every iteration of the MICE algorithm. At iteration 1, <span class="math inline">\(\rho(Y_1,Y_2)\)</span> is approximately 0.81, the value expected under independence of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>, conditional on <span class="math inline">\(X\)</span>. The influence of the complete records with both <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> observed percolates into the imputations, so that the chains slowly move into the direction of the population value of 0.7. The speed of convergence heavily depends on the number of missing cases. For 90% or 95% missing data, the streams are essentially flat after about 15â€“20 iterations. As the percentage of missing data increases, more and more iterations are needed before the true correlation of 0.7 trickles through. In the extreme cases with 100% missing data, the correlation <span class="math inline">\(\rho(Y_1,Y_2)\)</span> cannot be estimated due to lack of information in the data. In this case, the different streams do not converge at all, and wander widely within the Cauchyâ€“Schwarz bounds (0.6 to 1.0 here). But even here we could argue that the sampler has essentially converged. We could stop at iteration 200 and take the imputations from there. From a Bayesian perspective, this still would yield an essentially correct inference about <span class="math inline">\(\rho(Y_1,Y_2)\)</span>, being that it could be anywhere within the Cauchyâ€“Schwarz bounds. So even in this pathological case with 100% missing data, the results look sensible as long as we account for the wide variability.</p>
<p>The lesson we can learn from this simulation is that we should be careful about convergence in missing data problems with high correlations and high missing data rates. At the same time, observe that we really have to push the MICE algorithm to its limits to see the effect. Over 99% of real data will have lower correlations and lower missing data rates. Of course, it never hurts to do a couple of extra iterations, but my experience is that good results can often be obtained with a small number of iterations.</p>
</div>
<div id="performance" class="section level3">
<h3><span class="header-section-number">4.5.8</span> Performance</h3>
<p>Each conditional density has to be specified separately, so FCS requires some modeling effort on the part of the user. Most software provides reasonable defaults for standard situations, so the actual effort required may be small. A number of simulation studies provide evidence that FCS generally yields estimates that are unbiased and that possess appropriate coverage <span class="citation">(Brand <a href="references.html#ref-BRAND1999">1999</a>; Raghunathan et al. <a href="references.html#ref-RAGHUNATHAN2001">2001</a>; Brand et al. <a href="references.html#ref-BRAND2003">2003</a>; Tang et al. <a href="references.html#ref-TANG2005">2005</a>; Van Buuren et al. <a href="references.html#ref-VANBUUREN2006">2006</a>; Horton and Kleinman <a href="references.html#ref-HORTON2007">2007</a>; Yu, Burton, and Rivero-Arias <a href="references.html#ref-YU2007">2007</a>)</span>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-JM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="fcs-and-jm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
