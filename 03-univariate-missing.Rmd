# Univariate missing data {#ch:univariate}

> Statistics is a missing-data problem.
>
> --- Roderick J.A. Little

Chapter \[ch:mi\] described the theory of multiple imputation. This
chapter looks into ways of creating the actual imputations. In order to
avoid unnecessary complexities at this point, the text is restricted to
univariate missing data. The incomplete variable is called the *target*
variable. Thus, in this chapter there is only one variable with missing
values. The consequences of the missing data depend on the role of the
target variables within the complete-data model that is applied to the
imputed data.

There are many ways to create imputations, but only a few of those lead
to valid statistical inferences. This chapter outlines ways to check the
correctness of a procedure, and how this works out for selected
procedures. Most of the methods are designed to work under the
assumption that the relations within the missing parts are similar to
those in the observed parts, or more technically, the assumption of
ignorability. The chapter closes with a description of some alternatives
of what we might do when that assumption is suspect.

## How to generate multiple imputations

This section illustrates five ways to create imputations for a single
incomplete continuous target variable. We use dataset number 88 in
@HAND1994, which is also part of the `MASS`
library under the name `whiteside`. Mr.
Whiteside of the UK Building Research Station recorded the weekly gas
consumption (in 1000 cubic feet) and average external temperature (in
$^\circ\mathrm{C}$) at his own house in south-east England for two
heating seasons (1960 and 1961). The house thermostat was set at
20$^\circ\mathrm{C}$ throughout.

![Five ways to impute missing gas consumption for a temperature of
5$^\circ\mathrm{C}$: (a) no imputation; (b) predict; (c) predict +
noise; (d) predict + noise + parameter uncertainty; (e) two predictors;
(f) drawing from observed data.<span
data-label="fig:whiteside">](fig/ch3_gas1-1){width="\maxwidth"}

Figure \[fig:whiteside\]a plots the observed data. More gas is needed in
colder weeks, so there is an obvious relation in the data. The dataset
is complete, but for the sake of argument suppose that the gas
consumption in row 47 of the data is missing. The temperature at this
deleted observation is equal to 5$^\circ\mathrm{C}$. How would we create
multiple imputations for the missing gas consumption?

### Predict method

A first possibility is to calculate the regression line, and take the
imputation from the regression line. The estimated regression line is
equal to $y=5.49 - 0.29 x$, so the value at $x=5$ is
$5.49-0.29 \times 5 = 4.04$. Figure \[fig:whiteside\]b shows where the
imputed value is. This is actually the “best” value in the sense that it
is the most likely one under the regression model. However, even the
best value may differ from the actual (unknown) value. In fact, we are
uncertain about the true gas consumption. Predicted values, however, do
not portray this uncertainty, and therefore cannot be used as multiple
imputations.

### Predict + noise method {#sec:meth2}

We can improve upon the prediction method by adding an appropriate
amount of random noise to the predicted value. Let us assume that the
observed data are normally distributed around the regression line. The
estimated standard deviation in the Whiteside data is equal to 0.86
cubic feet. The idea is now to draw a random value from a normal
distribution with a mean of zero and a standard deviation of 0.86, and
add this value to the predicted value. The underlying assumption is that
the distribution of gas consumption of the incomplete observation is
identical to that in the complete cases.

We can repeat the draws to get multiple synthetic values around the
regression line. Figure \[fig:whiteside\]c illustrates five such drawn
values. On average, the synthetic values will be equal to the predicted
value. The variability in the values reflects that fact that we cannot
accurately predict gas consumption from temperature.

### Predict + noise + parameter uncertainty {#sec:meth3}

Adding noise is a major step forward, but not quite right. The method in
the previous section requires that the intercept, the slope and the
standard deviation of the residuals are known. However, the values of
these parameters are typically unknown, and hence must be estimated from
the data. If we had drawn a different sample from the same population,
then our estimates for the intercept, slope and standard deviation would
be different, perhaps slightly. The amount of extra variability is
strongly related to the sample size, with smaller samples yielding more
variable estimates.

The parameter uncertainty also needs to be included in the imputations.
There are two main methods for doing so. Bayesian methods draw the
parameters directly from their posterior distributions, whereas
bootstrap methods resample the observed data and re-estimate the
parameters from the resampled data.

Figure \[fig:whiteside\]d shows five sampled regression lines calculated
by Bayesian sampling. Imputed values are now defined as the predicted
value of the sampled line added with noise, as in Section \[sec:meth2\].

### A second predictor

The dataset actually contains a second predictor that indicates whether
the house was insulated or not. Incorporating this extra information
reduces the uncertainty of the imputed values.

Figure \[fig:whiteside\]e shows the same data, but now flagged according
to insulation status. Two regression lines are shown, one for the
insulated houses and the other for the non-insulated houses. It is clear
that less gas is needed after insulation. Suppose we know that the
external temperature is 5$^\circ\mathrm{C}$ *and* that the house was
insulated. How do we create multiple imputation given these two
predictors?

We apply the same method as in Section \[sec:meth3\], but now using the
regression line for the insulated houses. Figure \[fig:whiteside\]e
shows the five values drawn for this method. As expected, the
distribution of the imputed gas consumption has shifted downward.
Moreover, its variability is lower, reflecting that fact that gas
consumption can be predicted more accurately as insulation status is
also known.

### Drawing from the observed data

Figure \[fig:whiteside\]f illustrates an alternative method to create
imputations. As before, we calculate the predicted value at
5$^\circ\mathrm{C}$ for an insulated house, but now select a small
number of candidate donors from the observed data. The selection is done
such that the predicted values are close. We then randomly select one
donor from the candidates, and use the *observed* gas consumption that
belongs to that donor as the synthetic value. The figure illustrates the
candidate donors, not the imputations.

This method is known as *predictive mean matching*, and always finds
values that have been actually observed in the data. The underlying
assumption is that within the group of candidate donors gas consumption
has the same distribution in donors and receivers. The variability
between the imputations over repeated draws is again a reflection of the
uncertainty of the actual value.

### Conclusion

In summary, prediction methods are not suitable to create multiple
imputations. Both the inherent prediction error and the parameter
uncertainty should be incorporated into the imputations. Adding a
relevant extra predictor reduces the amount of uncertainty, and leads to
more efficient estimates later on. The text also highlights an
alternative that draws imputations from the observed data. The
imputation methods discussed in this chapter are all variations on this
basic idea.

## Imputation under the normal linear normal {#sec:linearnormal}

### Overview {#sec:linearoverview}

For univariate $Y$ we write lower-case $y$ for $Y$. Any predictors in
the imputation model are collected in $X$. Symbol $X_\mathrm{obs}$
indicates the subset of $n_1$ rows of $X$ for which $y$ is observed, and
$X_\mathrm{mis}$ is the complementing subset of $n_0$ rows of $X$ for
which $y$ is missing. The vector containing the $n_1$ observed data in
$y$ is denoted by $y_\mathrm{obs}$, and the vector of $n_0$ imputed
values in $y$ is indicated by $\dot y$. This section reviews four
different ways of creating imputations under the normal linear model.
The four methods are:

1.  *Predict*.
    ${\mbox{$\dot{y}$}}=\hat\beta_0+{\mbox{$X_\mathrm{mis}$}}\hat\beta_1$,
    where $\hat\beta_0$ and $\hat\beta_1$ are least squares estimates
    calculated from the observed data. Section \[sec:regimp\] named this
    regression imputation. In
    `mice` this method is
    available as method `norm.predict`.


2.  *Predict* + *noise*.
    ${\mbox{$\dot{y}$}}=\hat\beta_0+{\mbox{$X_\mathrm{mis}$}}\hat\beta_1+\dot\epsilon$,
    where $\dot\epsilon$ is randomly drawn from the normal distribution
    as $\dot\epsilon \sim N(0,\hat\sigma^2)$. Section \[sec:sri\] named
    this stochastic regression imputation. In
    `mice` this method is
    available as method `norm.nob`.

3.  *Bayesian multiple imputation*.
    ${\mbox{$\dot{y}$}}=\dot\beta_0+{\mbox{$X_\mathrm{mis}$}}\dot\beta_1+\dot\epsilon$,
    where $\dot\epsilon \sim N(0,\dot\sigma^2)$ and $\dot\beta_0$,
    $\dot\beta_1$ and $\dot\sigma$ are random draws from their posterior
    distribution, given the data. Section \[sec:meth3\] named this
    “predict + noise + parameters uncertainty.” The method is available
    as method `norm`.

4.  *Bootstrap multiple imputation*.
    ${\mbox{$\dot{y}$}}=\dot\beta_0+{\mbox{$X_\mathrm{mis}$}}\dot\beta_1+\dot\epsilon$,
    where $\dot\epsilon \sim N(0,\dot\sigma^2)$, and where
    $\dot\beta_0$, $\dot\beta_1$ and $\dot\sigma$ are the least squares
    estimates calculated from a bootstrap sample taken from the
    observed data. This is an alternative way to implement “predict +
    noise + parameters uncertainty.” The method is available as method
    `norm.boot`.

### Algorithms$^\spadesuit$ {#sec:linearalgorithm}

The calculations of the first two methods are straightforward and do not
need further explanation. This section describes the algorithms used to
introduce sampling variability into the parameters estimates of the
imputation model.

The Bayesian sampling draws $\dot\beta_0$, $\dot\beta_1$ and
$\dot\sigma$ from their respective posterior distributions. @BOX1973
[Section 2.7] explains the Bayesian theory behind the normal linear
model. We use the method that draws imputations under the normal linear
model using the standard noninformative priors for each of the
parameters. Given these priors, the required inputs are:

-   $y_\mathrm{obs}$, the $n_1 \times 1$ vector of observed data in the
    incomplete (or target) variable $y$;

-   $X_\mathrm{obs}$, the $n_1 \times q$ matrix of predictors of rows
    with observed data in $y$;

-   $X_\mathrm{mis}$, the $n_0 \times q$ matrix of predictors of rows
    with missing data in $y$.

The algorithm assumes that both $X_\mathrm{obs}$ and $X_\mathrm{mis}$
contain no missing values. Chapter \[ch:multivariate\] deals with the
case where $X_\mathrm{obs}$ and $X_\mathrm{mis}$ also could be
incomplete.

Algorithm \[alg:norm\] is adapted from @RUBIN1987 [p. 167], and is
implemented as the method `norm` (or,
equivalently, as the function
`mice.impute.norm()`) in the
`mice` package. Any drawn values
are identified with a dot above the symbol, so $\dot\beta$ is a value of
$\beta$ drawn from the posterior distribution. The algorithm uses a
ridge parameter $\kappa$ to evade problems with singular matrices. This
number should be set to a positive number close to zero, e.g.,
$\kappa=0.0001$. For some data, larger $\kappa$ may be needed. High
values of $\kappa$, e.g., $\kappa=0.1$, may introduce a systematic bias
toward the null, and should thus be avoided.

The bootstrap is a general method for estimating sampling variability
through resampling the data [@EFRON1993]. Algorithm \[alg:normboot\]
calculates univariate imputations by drawing a bootstrap sample from the
complete part of the data, and subsequently takes the least squares
estimates given the bootstrap sample as a “draw” that incorporates
sampling variability into the parameters [@HEITJAN1991]. Compared to the
Bayesian method, the bootstrap method avoids the Choleski decomposition
and does not need to draw from the $\chi^2$-distribution.

### Performance {#sec:perflin}

Which of these four imputation methods of Section \[sec:linearnormal\]
is best? In order to find out let us conduct a small simulation
experiment where we calculate the performance statistics introduced in
Section \[sec:quantifyingbias\]. We keep close to the original data by
assuming that $\beta_0=5.49$, $\beta_1= -0.29$ and $\sigma = 0.86$ are
the population values. These values are used to generate artificial data
with known properties.

  Method                                          Bias   % Bias   Coverage   CI Width    RMSE
  ------------------------------------------ --------- -------- ---------- ---------- -------
  `norm.predict`      0.0000      0.0      0.652      0.114   0.063
  `norm.nob`         -0.0001      0.0      0.908      0.226   0.064
  `norm`             -0.0001      0.0      0.951      0.314   0.066
  `norm.boot`        -0.0001      0.0      0.941      0.299   0.066
  Listwise deletion                             0.0001      0.0      0.946      0.251   0.063

  : Properties of $\beta_1$ under imputation of missing $y$ by five
  methods for the normal linear model ($n_\mathrm{sim} = 10000$).<span
  data-label="tab:linmody">

Table \[tab:linmody\] summarizes the results for the situation where we
have 50% completely random missing in $y$ and $m = 5$. All methods are
unbiased for $\beta_1$. The confidence interval of method
`norm.predict` is much too short, leading to
substantial undercoverage and $p$-values that are “too significant.”
This result confirms the problems already noted in Section \[sec:true\].
The `norm.nob` method performs better, but the
coverage of 0.908 is still too low. Methods
`norm` and
`norm.boot` and complete-case analysis are
correct. Complete-case analysis is a correct analysis here
[@LITTLE2002], and in fact the most efficient choice for this problem as
it yields the shortest confidence interval (cf. Section \[sec:when\]).
This result does not hold more generally. In realistic situations
involving more covariates multiple imputation will rapidly catch up and
pass complete-case analysis. Note that the RMSE values are uninformative
for separating correct and incorrect methods, and are in fact
misleading.

While method `norm.predict` is simple and
fast, the variance estimate is too low. Several methods have been
proposed to correct the estimate
[@LEE1994; @FAY1996; @RAO1996; @SCHAFER2000]. Though such methods
require special adaptation of formulas to calculate the variance, they
may be useful when the missing data are restricted to the outcome.

  Method                                          Bias   % Bias   Coverage   CI Width    RMSE
  ------------------------------------------ --------- -------- ---------- ---------- -------
  `norm.predict`     -0.1007     34.7      0.359      0.160   0.118
  `norm.nob`          0.0006      0.2      0.924      0.202   0.056
  `norm`              0.0075      2.6      0.955      0.254   0.058
  `norm.boot`        -0.0014      0.5      0.946      0.238   0.058
  Listwise deletion                            -0.0001      0.0      0.946      0.251   0.063

  : Properties of $\beta_1$ under imputation of missing $x$ by five
  methods for the normal linear model ($n_\mathrm{sim} = 10000$).<span
  data-label="tab:linmodx">

It is straightforward to adapt the simulations to other, perhaps more
interesting situations. Investigating the effect of missing data in the
explanatory $x$ instead of the outcome variable requires only a small
change in the function to create the missing data. Table \[tab:linmodx\]
displays the results. Method `norm.predict` is
now severely biased, whereas the other methods remain unbiased. The
confidence interval of `norm.nob` is still too
short, but less than in Table \[tab:linmody\]. Methods
`norm`, `norm.boot`
and listwise deletion are correct, in the sense that these are unbiased
and have appropriate coverage. Again, under the simulation conditions,
listwise deletion is the optimal analysis. Note that
`norm` is slightly biased, whereas method
`norm.boot` slightly underestimates the
variance. Both tendencies are small in magnitude. The RMSE values are
uninformative, and are only shown to illustrate that point.

We could increase the number of explanatory variables and the number of
imputations $m$ to see how much the average confidence interval width
would shrink. It is also easy to apply more interesting missing data
mechanisms, such as those discussed in Section \[sec:generateuni\]. Data
can be generated from skewed distributions, the sample size $n$ can be
varied and so on. Extensive simulation work is available
[@RUBIN1986B; @RUBIN1987].

### Generating MAR missing data {#sec:generateuni}

Just making random missing data is not always interesting. We obtain
more informative simulations if the missingness probability is a
function of the observed, and possibly of the unobserved, information.
This section considers some methods for creating missing data.

Let us first consider three methods to create missing data in artificial
data. The data are generated as 1000 draws from the bivariate normal
distribution $P(Y_1, Y_2)$ with means $\mu_1 = \mu_2
=5$, variances $\sigma_1^2 =\sigma_2^2 = 1$, and covariance
$\sigma_{12} = 0.6$. We assume that all values generated are positive.
Missing data in $Y_2$ can be created in many ways. Let $R_2$ be the
response indicator for $Y_2$. We study three examples, each of which
affects the distribution in different ways: $$\begin{aligned}
  \mathrm{MARRIGHT} &:& \mathrm{logit}(\Pr(R_2=0)) = -5 + Y_1\\
  \mathrm{MARMID} &:& \mathrm{logit}(\Pr(R_2=0)) = 0.75 - |Y_1-5|\\
  \mathrm{MARTAIL} &:& \mathrm{logit}(\Pr(R_2=0)) = -0.75 + |Y_1-5|\end{aligned}$$
where $\mathrm{logit}(p) = \log(p)-\log(1-p)$ with $0\leq p \leq 1$ is
the logit function. Its inverse $\mathrm{logit}^{-1}(x) =
\exp(x)/(1+\exp(x))$ is known as the logistic function.

Generating missing data under these models in
`R` can be done in three steps: calculate the
missingness probability of each data point, make a random draw from the
binomial distribution and set the corresponding observations to
`NA`. The following script creates missing
data according to MARRIGHT:

\
`  `\
`        `\
`         `\
`       `\
`   `\
`  `\
`     `

![Probability that $Y_2$ is missing as a function of the values of $Y_1$
under three models for the missing data.<span
data-label="fig:missingness">](fig/ch3_generateplot1-1){width="\maxwidth"}

![Box plot of $Y_2$ separated for the observed and missing parts under
three models for the missing data based on $n=10000$.<span
data-label="fig:threemars">](fig/ch3_generateplot2-1){width="\maxwidth"}

Figure \[fig:missingness\] displays the probability of being missing
under the three MAR mechanisms. All mechanisms yield approximately 50%
of missing data, but do so in very different ways. Figure
\[fig:threemars\] displays the distributions of $Y_2$ under the three
models. MARRIGHT deletes more high values, so the distribution of the
observed data shifts to the left. MARMID deletes more data in the
center, so the variance of the observed data grows, but the mean is not
affected. MARTAIL shows the reverse effect. The variance of observed
data reduces because the missing data occur in the tails.

These mechanisms are more extreme than we are likely to see in practice.
Not only is there a strong relation between $Y_1$ and $R_2$, but the
percentage of missing data is also quite high (50%). On the other hand,
if methods perform well under these data deletion schemes, they will
also do so in less extreme situations that are more likely to be
encountered in practice.

### MAR missing data generation in multivariate data {#sec:generatemulti}

Creating missing data from complete data is easy to do for simple
scenarios with one missing value per row. Things become more complicated
for multiple missing values per unit, as we need to be careful not to
delete any values that are needed to make the problem MAR.

@BRAND1999 [pp. 110–113] developed the following method for generating
non-monotone multivariate missing data in $p$ variables $Y_1,\dots,Y_p$.
We assume that $Y=(Y_1,\dots,Y_p)$ is initially completely known. The
method requires specification of

-   $\alpha$, the desired proportion of incomplete cases,

-   $R_\mathrm{pat}$, a binary $n_\mathrm{pat} \times p$ matrix
    defining $n_\mathrm{pat}$ allowed patterns of missing data, where
    all response patterns except $(0,0,\dots,0)$ and $(1,1,\dots,1)$ may
    occur,

-   $f=(f_{(1)},\dots,f_{(n_\mathrm{pat})})$, a vector containing
    the relative frequencies of each pattern, scaled such that
    $\sum_{s}^{n_\mathrm{pat}} f_{(s)} = 1$,

-   $P(R|Y)=(P(R_{(1)}|Y{(1)}),\dots,P(R_{(n_\mathrm{pat})}|Y_{(n_\mathrm{pat})}))$,
    a set of $n_\mathrm{pat}$ response probability models, one for
    each pattern.

The general procedure is as follows: Each case is allocated to one of
$n_\mathrm{pat}$ candidate blocks using a random draw from the
multinomial distribution with probabilities
$f_{(1)},\dots,f_{(n_\mathrm{pat})}$. Within the $s^\mathrm{th}$
candidate block, a subgroup of $\alpha nf_{(s)}$ cases is made
incomplete according to pattern $R_{(s)}$ using the missing data model
$P(R_{(s)}|Y_{(s)})$, where $s=1,\dots,n_\mathrm{pat}$. The procedure
results in approximately $\alpha$ incomplete cases, that are distributed
over the allowed response patterns. If the missing data are to be MAR,
then the missing variables in the $s^\mathrm{th}$ pattern should not
influence the missingness probability defined by the missing data model
$P(R_{(s)}|Y_{(s)}$ for block $s$.

The `ampute()` function in
`mice` implements the method. For example, we
can create 50% missing data in both $Y_1$ and $Y_2$ according to a
MARRIGHT scenario by

`     `\
`    `

      V1   V2
    4.91 4.91

As expected, the means in the amputed data are lower than in the
complete data. It is possible to inspect the distributions of the
observed data more closely by
`md.pattern(amp$amp)`,
`bwplot(amp)` and
`xyplot(amp)`. Many options are available that
allows the user to tailor the missing data patterns to the data at hand.
See @SCHOUTEN2018 for details.

### Conclusion

Tables \[tab:linmody\] and \[tab:linmodx\] show that methods
`norm.predict` (regression imputation) and
`norm.nob` (stochastic regression imputation)
fail in terms of understating the uncertainty in the imputations. If the
missing data occur in $y$ only, then it is possible to correct the
variance formulas of method `norm.predict`.
However, if the missing data occur in $X$,
`norm.predict` is severely biased, so then
variance correction is not useful. Methods
`norm` and
`norm.boot` account for the uncertainty of the
imputation model provide statistically correct inferences. For missing
$y$, the efficiency of these methods is less than theoretically
possible, presumably due to simulation error.

It is always better to include parameter uncertainty, either by the
Bayesian or the bootstrap method. The effect of doing so will diminish
with increasing sample size (Exercise \[ex:sampling\]), so for estimates
based on a large sample one may opt for the simpler
`norm.nob` method if speed of calculation is
at premium. Note that in subgroup analyses, the large-sample requirement
applies to the subgroup size, and not to the total sample size.

## Imputation under non-normal distributions {#sec:nonnormal}

### Overview {#overview}

The imputation methods discussed in Section \[sec:linearnormal\] produce
imputations drawn from a normal distribution. In practice the data could
be skewed, long tailed, non-negative, bimodal or rounded, to name some
deviations from normality. This creates an obvious mismatch between
observed and imputed data which could adversely affect the estimates of
interest.

The effect of non-normality is generally small for measures that rely on
the center of the distribution, like means or regression weights, but it
could be substantial for estimates like a variance or a percentile. In
general, normal imputations appear to be robust against violations of
normality. @DEMIRTAS2008C found that flatness of the density, heavy
tails, non-zero peakedness, skewness and multimodality do not appear to
hamper the good performance of multiple imputation for the mean
structure in samples $n>400$, even for high percentages (75%) of missing
data in one variable. The variance parameter is more critical though,
and could be off-target in smaller samples.

One approach is to transform the data toward normality before
imputation, and back-transform them after imputation. A beneficial side
effect of transformation is that the relation between $x$ and $y$ may
become closer to a linear relation. Sometimes applying a simple function
to the data, like the logarithmic or inverse transform, is all that is
needed. More generally, the transformation could be made to depend on
known covariates like age and sex, for example as done in the LMS model
[@COLE1992] or the GAMLSS model [@RIGBY2005].

warns that application of tricks to make the distribution of skewed
variables closer to normality (e.g., censoring, truncation,
transformation) may make matters worse. Censoring (rounding a disallowed
value to the nearest allowed value) and truncation (redrawing a
disallowed value until it is within the allowed range) can change both
the mean and variability in the data. Transformations may fail to
achieve near-normality, and even if that succeeds, bivariate relations
may be affected when imputed by a method that assumes normality. The
examples of Von Hippel are somewhat extreme, but they do highlight the
point that simple fixes to achieve normality are limited by what they
can do.

There are two possible strategies to progress. The first is to use
predictive mean matching. Section \[sec:pmm\] will describe this
approach in more detail. The other strategy is to model the non-normal
data, and to directly draw imputations from those models. @LIU1995
proposed methods for drawing imputations under the $t$-distribution
instead of the normal. @HE2006 created imputations by drawing from
Tukey’s $gh$-distribution, which can take many shapes. @DEMIRTAS2008
investigated the behavior of methods for drawing imputation from the
Beta and Weibull distributions. Likewise, @DEMIRTAS2008B took draws from
Fleishman polynomials, which allows for combinations of left and right
skewness with platykurtic and leptokurtic distributions.

The GAMLSS method [@RIGBY2005; @STASINOPOULOS2017] extends both the
generalized linear model and the generalized additive model. A unique
feature of GAMLSS is its ability to specify a (possibly nonlinear) model
for each of the parameters of the distribution, thus giving rise to an
extremely flexible toolbox that can be used to model almost any
distribution. The `gamlss` package contains
over 60 built-in distributions. Each distribution comes with a function
to draw random variates, so once the `gamlss`
model is fitted, it can also be used to draw imputations. The first
edition of this book showed how to construct a new univariate imputation
function that `mice` could call.
This is not needed any more. @DEJONG2012 and @DEJONG2016 developed a
series of imputation methods based on GAMLSS, so it is now easy to
perform multiple imputation under variety of distributions. The
`ImputeRobust` package [@SALFRAN2017],
implements various `mice` methods for
continuous data: `gamlss` (normal),
`gamlssJSU` (Johnson’s SU),
`gamlssTF` ($t$-distribution) and
`gamlssGA` (gamma distribution). The following
section demonstrates the use of the package.

### Imputation from the $t$-distribution {#sec:tdist}

We illustrate imputation from the $t$-distribution. The $t$-distribution
is favored for more robust statistical modeling in a variety of settings
[@LANGE1989]. observed unexplained kurtosis in the distribution of head
circumference in children. @RIGBY2006 fitted a $t$-distribution to these
data, and observed a substantial improvement of the fit.

![Measured head circumference of 755 Dutch boys aged 1–2 years
[@FREDRIKS2000B].<span
data-label="fig:hc">](fig/ch3_linhc1-1){width="\maxwidth"}

Figure \[fig:hc\] plots the data for Dutch boys aged 1–2 years. Due to
the presence of several outliers, the $t$-distribution with 6.7 degrees
of freedom fits the data substantially better than the normal
distribution (Akaike Information Criterion (AIC): 2974.5 (normal model)
versus 2904.3 ($t$-distribution). If the outliers are genuine data, then
the $t$-distribution should provide imputations that are more realistic
than the normal.

We create a synthetic dataset by imputing head circumference of the same
755 boys. Imputation is easily done with the following steps: append the
data with a duplicate, create missing data in
`hc` and run
`mice()` calling the
`gamlssTF` method as follows:

\
\
\
`          `\
`   `\
`      `\
`  `\
`  `\
`   `\
`           `\
`              `\
`  `

![Fully synthetic data of head circumference of 755 Dutch boys aged 1–2
years using a $t$-distribution.<span
data-label="fig:hcimp">](fig/ch3_linhc3-1){width="\maxwidth"}

Figure \[fig:hcimp\] is the equivalent of Figure \[fig:hc\], but now
calculated from the synthetic data. Both configurations are similar. As
expected, some outliers also occur in the imputed data, but these are a
little less extreme than in the observed data due to the smoothing by
the $t$-distribution. The estimated degrees of freedom varies over
replications, and appears to be somewhat larger than the value of 6.7
estimated from the observed data. For this replication, it is larger
(11.5). The distribution of the imputed data is better behaved compared
to the observed data. The typical rounding patterns seen in the real
measurements are not present in the imputed data. Though these are small
differences, they may be of relevance in particular analyses.

## Predictive mean matching {#sec:pmm}

### Overview {#overview-1}

Predictive mean matching calculates the predicted value of target
variable $Y$ according to the specified imputation model. For each
missing entry, the method forms a small set of candidate donors
(typically with 3, 5 or 10 members) from all complete cases that have
predicted values closest to the predicted value for the missing entry.
One donor is randomly drawn from the candidates, and the observed value
of the donor is taken to replace the missing value. The assumption is
the distribution of the missing cell is the same as the observed data of
the candidate donors.

Predictive mean matching is an easy-to-use and versatile method. It is
fairly robust to transformations of the target variable, so imputing
$\log(Y)$ often yields results similar to imputing $\exp(Y)$. The method
also allows for discrete target variables. Imputations are based on
values observed elsewhere, so they are realistic. Imputations outside
the observed data range will not occur, thus evading problems with
meaningless imputations (e.g., negative body height). The model is
implicit [@LITTLE2002], which means that there is no need to define an
explicit model for the distribution of the missing values. Because of
this, predictive mean matching is less vulnerable to model
misspecification than the methods discussed in Sections
\[sec:linearnormal\] and \[sec:nonnormal\].

![Robustness of predictive mean matching (right) relative to imputation
under the linear normal model (left).<span
data-label="fig:misspecify">](fig/ch3_figmisspecify-1){width="\maxwidth"}

Figure \[fig:misspecify\] illustrates the robustness of predictive mean
matching relative to the normal model. The figure displays the body mass
index (BMI) of children aged 0–2 years. BMI rapidly increases during the
first half year of life, has a peak around 1 year and then slowly drops
at ages when the children start to walk. The imputation model is,
however, incorrectly specified, being linear in age. Imputations created
under the normal model display in an incorrect slowly rising pattern,
and contain several implausible values. In contrast, the imputations
created by predictive mean matching follow the data quite nicely, even
though the predictive mean itself is clearly off-target for some of the
ages. This example shows that predictive mean matching is robust against
misspecification, where the normal model is not.

Predictive mean matching is an example of a hot deck method, where
values are imputed using values from the complete cases matched with
respect to some metric. The expression “hot deck” literally refers to a
pack of computer control cards containing the data of the cases that are
in some sense close. Reviews of hot deck methods can be found in
@FORD1983, @BRICK1996, @KOLLER2009, @ANDRIDGE2010 and @DEWAAL2011 [pp.
249–255, 349–355].

![Selection of candidate donors in predictive mean matching with the
stochastic matching distance.<span
data-label="fig:pmm">](fig/ch3_pmmfigure-1){width="\maxwidth"}

Figure \[fig:pmm\] is an illustration of the method using the
`whiteside` data. The predictor is equal to
5$^\circ\mathrm{C}$ and the bandwidth is 1.2. The thick
gray line indicates the area of the target variable where
matches should be sought. The gray part of the figure are
considered fixed. The red line correspond to one random draw of the line
parameters to incorporate sampling uncertainty. The two light-red bands
indicate the area where matches are permitted. In this particular
instance, five candidate donors are found, four from the subgroup “after
insulation” and one from the subgroup “before insulation.” The last step
is to make a random draw among these five candidates. The red parts in
the figure will vary between different imputed datasets, and thus the
set of candidates will also vary over the imputed datasets.

The data point at coordinate (10.2, 2.6) is one of the candidate donors.
This point differs from the incomplete unit in both temperature and
insulation status, yet it is selected as a candidate donor. The
advantage of including the point is that closer matches in terms of the
predicted values are possible. Under the assumption that the
distribution of the target in different bands is similar, including
points from different bands is likely to be beneficial.

### Computational details$^\spadesuit$ {#sec:pmmcomputation}

Various metrics are possible to define the distance between the cases.
The predictive mean matching metric was proposed by @RUBIN1986A and
@LITTLE1988. This metric is particularly useful for missing data
applications because it is optimized for each target variable
separately. The predicted value only needs to be a convenient one-number
summary of the important information that relates the covariates to the
target. Calculation is straightforward, and it is easy to include
nominal and ordinal variables.

Once the metric has been defined, there are various ways to select the
donor. Let $\hat y_i$ denote the predicted value of the rows with an
observed $y_i$ where $i=1,\dots,n_1$. Likewise, let $\hat y_j$ denote
the predicted value of the rows with missing $y_j$ where
$j=1,\dots,n_0$. @ANDRIDGE2010 distinguish four methods:

1.  Choose a threshold $\eta$, and take all $i$ for which $|\hat
        y_i-\hat y_j|<\eta$ as candidate donors for imputing $j$.
    Randomly sample one donor from the candidates, and take its $y_i$ as
    replacement value.

2.  Take the closest candidate, i.e., the case $i$ for which
    $|\hat
        y_i-\hat y_j|$ is minimal as the donor. This is known as
    “nearest neighbor hot deck,” “deterministic hot deck” or
    “closest predictor.”

3.  Find the $d$ candidates for which $|\hat y_i-\hat y_j|$ is
    minimal, and sample one of them. Usual values for $d$ are 3, 5
    and 10. There is also an adaptive method to specify the number of
    donors [@SCHENKER1996].

4.  Sample one donor with a probability that depends on
    $|\hat y_i-\hat y_j|$ [@SIDDEQUE2008].

In addition, it is useful to distinguish four types of matching:

1.  *Type 0*: $\hat y={\mbox{$X_\mathrm{obs}$}}\hat\beta$ is matched to
    $\hat y_j={\mbox{$X_\mathrm{mis}$}}\hat\beta$;

2.  *Type 1*: $\hat y={\mbox{$X_\mathrm{obs}$}}\hat\beta$ is matched to
    $\dot y_j={\mbox{$X_\mathrm{mis}$}}\dot\beta$;

3.  *Type 2*: $\dot y={\mbox{$X_\mathrm{obs}$}}\dot\beta$ is matched to
    $\dot y_j={\mbox{$X_\mathrm{mis}$}}\dot\beta$;

4.  *Type 3*: $\dot y={\mbox{$X_\mathrm{obs}$}}\dot\beta$ is matched to
    $\ddot y_j={\mbox{$X_\mathrm{mis}$}}\ddot\beta$.

Here $\hat\beta$ is the estimate of $\beta$, while $\dot\beta$ is a
value randomly drawn from the posterior distribution of $\beta$. Type 0
matching ignores the sampling variability in $\hat\beta$, leading to
improper imputations. Type 2 matching appears to solve this. However, it
is insensitive to the process of taking random draws of $\beta$ if there
are only a few variables. In the extreme case, with a single $X$, the
set of candidate donors based on $|\dot y_i-\dot y_j|$ remains unchanged
under different values of $\dot\beta$, so the same donor(s) get selected
too often. Type 1 matching is a small but nifty adaptation of the
matching distance that seems to alleviate the problem. The difference
with Type 0 and Type 2 matching is that in Type 1 matching only
${\mbox{$X_\mathrm{mis}$}}\dot\beta$ varies stochastically and does not
cancel out any more. As a result $\dot\eta$ incorporates
between-imputation variation. Type 3 matching creates two draws for
$\beta$, one for the donor set and one for the recipient set. In
retrospect, it is interesting to note that Type 1 matching was already
described by @LITTLE1988 [eq. 4]. It disappeared from the literature,
only to reappear two decades later in the works of @KOLLER2009 [p. 43]
and @WHITE2011 [p. 383].

Algorithm \[alg:pmm\] provides the steps used in predictive mean
matching using Bayesian parameter draws for $\beta$. It is possible to
create the bootstrap version of this algorithm that will also evade the
need to draw $\beta$ along the same lines as Algorithm \[alg:normboot\].
Given that the number of candidate donors and the model for the mean is
provided by the user, the algorithm does not need an explicit
specification of the distribution.

@MORRIS2014 suggested a variation called *local residuals draws*. Rather
than taking the observed value of the donor, this method borrows the
residual from the donor, and adds that to the predicted value from the
target case. Thus, imputations are not equal to observed values, and can
extend beyond the range of the observed data. This may address concerns
about variability of imputations.

### Number of donors

There are different strategies for defining the set and number of
candidate donors. Setting $d = 1$ is generally considered to be too low,
as it may reselect the same donor over and over again. Predictive mean
matching performs very badly when $d$ is small and there are lots of
ties for the predictors among the individuals to be imputed. The reason
is that the tied individuals all get the same imputed value in each
imputed dataset when $d = 1$ (Ian White, personal communication).
Setting $d$ to a high value (say $n/10$) alleviates the duplication
problem, but may introduce bias since the likelihood of bad matches
increases. @SCHENKER1996 evaluated $d = 3$, $d = 10$ and an adaptive
scheme. The adaptive method was slightly better than using a fixed
number of candidates, but the differences were small. compared various
settings for $d$, and found that $d = 5$ and $d = 10$ generally provided
the best results. found that $d = 5$ may be too high for sample size
lower than $n = 100$, and suggested setting $d = 1$ for better point
estimates for small samples. @GAFFERT2016 explored scenarios in which
candidate donors have different probabilities to be drawn, where the
probability depends on the distance between the donor and recipient
cases. As all observed cases can be donors in this scenario, there is no
need to specify $d$. Instead a closeness parameter needs to be
specified, and this was made adaptive to the data. An advantage of using
all donors is that the variance of the imputations can be corrected by
the Parzen correction, which alleviates concerns about insufficient
variability of the imputes. Their simulations showed that with a small
sample ($n = 10$), the adaptive method is clearly superior to methods
with a fixed donor pool. The method is available in
`mice` as the
`midastouch` method. There is also a separate
`midastouch` package in
`R`. Related work can be found in @TUTZ2015.

The default in `mice` is $d = 5$, and
represents a compromise. The above results suggest that an adaptive
method for setting $d$ could improve small sample behavior. Meanwhile,
the number of donors can be changed through the
`donors` argument.

lrrrrrr Method & & Bias & % Bias & Coverage & CI Width &
RMSE\
Missing $y$, $n = 50$ & $d$\
`pmm` & 1 & 0.016 & 5.4 & 0.884 & 0.252 &
0.071\
`pmm` & 3 & 0.028 & 9.7 & 0.890 & 0.242 &
0.070\
`pmm` & 5 & 0.039 &13.6 & 0.876 & 0.241 &
0.075\
`pmm` &10 & 0.065 &22.4 & 0.806 & 0.245 &
0.089\
\
Missing $x$\
`pmm` & 1 &-0.002 & 0.8 & 0.916 & 0.223 &
0.063\
`pmm` & 3 & 0.002 & 0.9 & 0.931 & 0.228 &
0.061\
`pmm` & 5 & 0.008 & 2.8 & 0.938 & 0.237 &
0.062\
`pmm` &10 & 0.028 & 9.6 & 0.946 & 0.261 &
0.067\
\
Listwise deletion & & 0.000 & 0.0 & 0.946 & 0.251 & 0.063\
\
\
Missing $y$, $n = 50$ & $\kappa$\
`midastouch` &auto& 0.013 & 4.5 & 0.920 &
0.265 & 0.066\
`midastouch` & 2& 0.032 &11.1 & 0.917 & 0.273
& 0.068\
`midastouch` & 3& 0.018 & 6.2 & 0.927 & 0.261
& 0.064\
`midastouch` & 4& 0.012 & 4.1 & 0.926 & 0.260
& 0.064\
\
Missing $x$\
`midastouch` &auto&-0.003&0.9 & 0.932 & 0.241
& 0.060\
`midastouch` & 2& 0.013 & 4.4 & 0.959 & 0.264
& 0.059\
`midastouch` & 3& 0.000 & 0.2 & 0.947 & 0.245
& 0.058\
`midastouch` & 4&-0.004 & 1.4 & 0.940 & 0.237
& 0.058\
\
Listwise deletion & & 0.000 & 0.0 & 0.946 & 0.251 & 0.063\
\
\
Missing $y$, $n = 1000$ & $d$\
`pmm` & 1 & 0.001 & 0.2 & 0.929 & 0.056 &
0.014\
`pmm` & 3 & 0.001 & 0.4 & 0.950 & 0.056 &
0.013\
`pmm` & 5 & 0.002 & 0.6 & 0.951 & 0.055 &
0.013\
`pmm` & 10& 0.003 & 1.2 & 0.932 & 0.054 &
0.013\
\
Missing $x$\
`pmm` & 1 & 0.000 & 0.2 & 0.926 & 0.041 &
0.011\
`pmm` & 3 & 0.000 & 0.1 & 0.933 & 0.041 &
0.011\
`pmm` & 5 & 0.000 & 0.1 & 0.937 & 0.042 &
0.011\
`pmm` & 10& 0.000 & 0.1 & 0.928 & 0.042 &
0.011\
\
Listwise deletion & & 0.000 & 0.1 & 0.955 & 0.050 & 0.012\

Table \[tab:pmm\] repeats the simulation experiment done in Tables
\[tab:linmody\] and \[tab:linmodx\] for predictive mean matching for
three different choices of the number $d$ of candidate donors. Results
are given for $n = 50$ and $n = 1000$. For $n = 50$ we find that
$\beta_1$ is increasingly biased towards the null for larger $d$.
Because of the bias, the coverage is lower than nominal. For missing $x$
the bias is much smaller. Setting $d$ to a lower value, as recommended
by @KLEINKE2017, improves point estimates, but the magnitude of the
effect depends on whether the missing values occur in $x$ or $y$. For
the sample size $n = 1000$ predictive mean matching appears well
calibrated for $d = 5$ for missing data in $y$, and has slight
undercoverage for missing data in $x$. Note that Table \[tab:pmm\] in
the first edition of this book presented incorrect information because
it had erroneously imputed the data by `norm`
instead of `pmm`.

### Pitfalls

The obvious danger of predictive mean matching is the duplication of the
same donor value many times. This problem is more likely to occur if the
sample is small, or if there are many more missing data than observed
data in a particular region of the predicted value. Such unbalanced
regions are more likely if the proportion of incomplete cases is high,
or if the imputation model contains variables that are very strongly
related to the missingness. For small samples the donor pool size can be
reduced, but be aware that this may not work if there are only a few
predictors.

The traditional method does not work for a small number of predictors.
@HEITJAN1991 report that for just two predictors the results were
“disastrous.” The cause of the problem appears to be related to their
use of Type 0 matching. The default in `mice`
is Type 1 matching, which works better for small number of predictors.
The setting can be changed to Type 0 or Type 2 matching through the
`matchtype` argument.

Predictive mean matching is no substitute for sloppy modeling. If the
imputation model is misspecified, performance can become poor if there
are strong relations in the data that are not modeled [@MORRIS2014]. The
default imputation model in `mice` consists of
a linear main effect model conditional on all other variables, but this
may be inadequate in the presence of strong nonlinear relations. More
generally, any terms appearing in the complete-data model need to be
accounted for in the imputation model. @MORRIS2014 advise to spend
efforts on specifying the imputation model correctly, rather than
expecting predictive mean matching to do the work.

### Conclusion

Predictive mean matching with $d=5$ is the default in
`mice()` for continuous data. The method is
robust against misspecification of the imputation model, yet performs as
well as theoretically superior methods. In the context of missing
covariate data, @MARSHALL2010 concluded that predictive mean matching
“produced the least biased estimates and better model performance
measures.” Another simulation study that addressed skewed data concluded
that predictive mean matching “may be the preferred approach provided
that less than 50% of the cases have missing data and the missing data
are not MNAR” [@MARSHALL2010B]. @KLEINKE2017 found that the method works
well across a wide variety of scenarios, but warned the default cannot
address severe skewness or small samples.

The method works best with large samples, and provides imputations that
possess many characteristics of the complete data. Predictive mean
matching cannot be used to extrapolate beyond the range of the data, or
to interpolate within the range of the data if the data at the interior
are sparse. Also, it may not perform well with small datasets. Bearing
these points in mind, predictive mean matching is a great all-around
method with exceptional properties.

## Classification and regression trees {#sec:cart}

### Overview {#sec:cartoverview}

Classification and regression trees (CART) [@BREIMAN1984] are a popular
class of machine learning algorithms. CART models seek predictors and
cut points in the predictors that are used to split the sample. The cut
points divide the sample into more homogeneous subsamples. The splitting
process is repeated on both subsamples, so that a series of splits
defines a binary tree. The target variable can be discrete
(classification tree) or continuous (regression tree).

![Regression tree for predicting gas consumption. The left-hand plot
displays the binary tree, whereas the right-hand plot identifies the
groups at each end leaf in the data.<span
data-label="fig:cart">](fig/ch3_cart-1){width="\maxwidth"}

Figure \[fig:cart\] illustrates a simple CART solution for the
`whiteside` data. The left-hand side contains
the optimal binary tree for predicting gas consumption from temperature
and insulation status. The right-hand side shows the scatterplot in
which the five groups are labeled by their terminal nodes.

CART methods have properties that make them attractive for imputation:
they are robust against outliers, can deal with multicollinearity and
skewed distributions, and are flexible enough to fit interactions and
nonlinear relations. Furthermore, many aspects of model fitting have
been automated, so there is “little tuning needed by the imputer”
[@BURGETTE2010].

The idea of using CART methods for imputation has been suggested by a
wide variety of authors in a variety of ways. See @SAAR2007 for an
introductory overview. Some investigators [@HEYAN2006; @VATEEKUL2009]
simply fill in the mean or mode. The majority of tree-based imputation
methods use some form of single imputation based on prediction
[@BARCENA2000; @CONVERSANO2003; @SICILIANO2006; @CREEL2006; @ISHWARAN2008; @CONVERSANO2009].
Multiple imputation methods have been developed by @HARRELL2001, who
combined it with optimal scaling of the input variables, by @REITER2005B
and by @BURGETTE2010. @WALLACE2010 present a multiple imputation method
that averages the imputations to produce a single tree and that does not
pool the variances. @PARKER2010 investigates multiple imputation methods
for various unsupervised and supervised learning algorithms.

The `missForest` method [@STEKHOVEN2011]
successfully used regression and classification trees to predict the
outcomes in mixed continuous/categorical data.
`MissForest` is popular, presumably because it
produces a *single* complete dataset, which at the same time is the
reason why it fails as a scientific method. The
`missForest` method does not account for the
uncertainty caused by the missing data, treats the imputed data as if
they were real (which they are not), and thus invents information. As a
consequence, $p$-values calculated after application of
`missForest` will be more significant than
they actually are, confidence intervals will be shorter than they
actually are, and relations between variables will be stronger than they
actually are. These problems worsen as more missing values are imputed.
Unfortunately, comparisons studies that evaluate only accuracy, such as
@WALJEE2013, will fail to detect these problems.

As a alternative, multiple imputations can be created using the tree in
Figure \[fig:cart\]. For a given temperature and insulation status,
traverse the tree and find the appropriate terminal node. Form the donor
group of all observed cases at the terminal node, randomly draw a case
from the donor group, and take its reported gas consumption as the
imputed value. The idea is identical to predictive mean matching
(cf. Section \[sec:pmm\]), where the “predictive mean” is now calculated
by a tree model instead of a regression model. As before, the parameter
uncertainty can be incorporated by fitting the tree on a bootstrapped
sample.

Algorithm \[alg:cart\] describes the major steps of an algorithm for
creating imputations using a classification or regression tree. There is
considerable freedom at step 2, where the tree model is fitted to the
training data
$({\mbox{$\dot y_\mathrm{obs}$}},{\mbox{$\dot X_\mathrm{obs}$}})$. It
may be useful to fit the tree such that the number of cases at each node
is equal to some pre-set number, say 5 or 10. The composition of the
donor groups will vary over different bootstrap replications, thus
incorporating sampling uncertainty about the tree.

Multiple imputation methodology using trees has been developed by
@BURGETTE2010, @SHAH2014 and @DOOVE2014. The main motivation given in
these papers was to improve our ability to account for interactions and
other non-linearities, but these are generic methods that apply to both
continuous and categorical outcomes and predictors. @BURGETTE2010 used
the `tree` package, and showed that the CART
results for recovering interactions were uniformly better than standard
techniques. @SHAH2014 applied random forest techniques to both
continuous and categorical outcomes, which produced more efficient
estimates than standard procedures. The techniques are available as
methods `rfcat` and
`rfcont` in the
`CALIBERrfimpute` package. @DOOVE2014
independently developed a similar set of routines building on the
`rpart` [@THERNEAU2017] and
`randomForest` [@LIAW2002] packages. Methods
`cart` and `rf` are
part of `mice`.

A recent development is the growing interest from the machine learning
community for the idea of multiple imputation. The problem of imputing
missing values has now been discovered by many, but unfortunately nearly
all new algorithms produce single imputations. An exception is the paper
by @SOVILJ2016, who propose the *extreme learning machine* using
conditional Gaussian mixture models to generate multiple imputations. It
is a matter of time before researchers realize the intimate connections
between multiple imputation and ensemble learning, so that more work
along these lines may follow.

## Categorical data {#sec:categorical}

### Generalized linear model {#sec:categoricaloverview}

Imputation of missing categorical data is possible under the broad class
of generalized linear models [@MCCULLAGH1989]. For incomplete binary
variables we use *logistic regression*, where the outcome probability is
modeled as

$$\Pr(y_i=1|X_i, \beta) = \frac{\exp(X_i\beta)}{1+\exp(X_i\beta)}$$

A categorical variable with $K$ unordered categories is imputed under
the *multinomial logit model*
$$\Pr(y_i=k|X_i, \beta) = \frac{\exp(X_i\beta_k)}{\sum_{k=1}^K \exp(X_i\beta_k)}$$
for $k=1,\dots,K$, where $\beta_k$ varies over the categories and where
$\beta_1=0$ to identify the model. A categorical variable with $K$
ordered categories is imputed by the *ordered logit model*, or
*proportional odds model*
$$\Pr(y_i\leq k|X_i, \beta, \tau_k) = \frac{\exp(\tau_k - X_i\beta)}
 {1 + \exp(\tau_k - X_i\beta)}$$ with the slope $\beta$ is identical
across categories, but the intercepts $\tau_k$ differ. For
identification, we set $\tau_1=0$. The probability of observing category
$k$ is written as $$\Pr(y_i = k|X_i) =
  \Pr(y_i\leq k|X_i) -
  \Pr(y_i\leq k-1|X_i)$$ where the model parameters $\beta$, $\tau_k$
and $\tau_{k-1}$ are suppressed for clarity. @SCOTTLONG1997 is a very
readable introduction to these methods. The practical application of
these techniques in `R` is treated in
@AITKIN2009. The general idea is to estimate the probability model on
the subset of the observed data, and draw synthetic data according to
the fitted probabilities to impute the missing data. The parameters are
typically estimated by iteratively reweighted least squares. As before,
the variability of the model parameters $\beta$ and
$\tau_2,\dots,\tau_K$ introduces additional uncertainty that needs to be
incorporated into the imputations.

Algorithm \[alg:binary\] provides the steps for an approximate Bayesian
imputation method using logistic regression. The method assumes that the
parameter vector $\beta$ follows a multivariate normal distribution.
Although this is true in large samples, the distribution can in fact be
far from normal for modest $n_1$, for large $q$ or for predicted
probabilities close to 0 or 1. The procedure is also approximate in the
sense that it does not draw the estimated covariance $V$ matrix. It is
possible to define an explicit Bayesian sampling for drawing $\beta$ and
$V$ from their exact posteriors. This method is theoretically
preferable, but as it requires more elaborate modeling, it does not
easily extend to other regression situations. In
`mice` the algorithm is
implemented as the method `logreg`.

It is easy to construct a bootstrap version that avoids some of the
difficulties in Algorithm \[alg:binary\]. Prior to estimating
$\hat\beta$, we include a step that draws a bootstrap sample from
$Y_\mathrm{obs}$ and $X_\mathrm{obs}$. Steps 2–5 can then be replaced by
equating $\dot\beta=\hat\beta$.

The algorithms for imputation of variables with more than two categories
follow the same structure. In
`mice` the multinomial logit
model in method `polyreg` is estimated by the
`nnet::multinom()` function in the
`nnet` package. The ordered logit model in
method `polr` is estimated by the
`polr()` function of the
`MASS` package. Even though the ordered model
uses fewer parameters, it is often more difficult to estimate. In cases
where `MASS::polr()` fails to converge,
`nnet::multinom()` will take over its duties.
See @VENABLES2002 for more details on both functions.

### Perfect prediction$^\spadesuit$

There is a long-standing technical problem in models with categorical
outcomes, known as *separation* or *perfect prediction*
[@ALBERT1984; @LESAFFRE1989]. The standard work by @HOSMER2000 [pp.
138–141] discussed the problem, but provided no solution. The problem
occurs, for example, when predicting the presence of a disease from a
set of symptoms. If one of the symptoms (or a combination of symptoms)
always leads to the disease, then we can perfectly predict the disease
for any patient who has the symptom(s).

  --------- ----- -----

  Disease     Yes    No
  Yes         100   100
  No            0   100
  Unknown     100   100
  --------- ----- -----

  : Artificial data demonstrating complete separation. Adapted from
  @WHITE2010B.<span data-label="tab:perfectpred">

Table \[tab:perfectpred\] contains an artificial numerical example.
Having the symptom always implies the disease, so knowing that the
patient has the symptom will allow perfect prediction of the disease
status. When such data are analyzed, most software will print out a
warning message and produce unusually large standard errors.

Now suppose that in a new group of 200 patients (100 in each symptom
group) we know only the symptom and impute disease status. Under MAR, we
should impute all 100 cases with the symptom to the diseased group, and
divide the 100 cases without the symptom randomly over the diseased and
non-diseased groups. However, this is not what happens in Algorithm
\[alg:binary\]. The estimate of $V$ will be very large as a result of
separation. If we naively use this $V$ then $\dot\beta$ in step 5
effectively covers both positive and negative values equally likely.
This results in either correctly 100 imputations in
`Yes` or incorrectly 100 imputations in
`No`, thereby resulting in bias in the disease
probability.

The problem has recently gained attention. There are at least six
different approaches to perfect prediction:

1.  Eliminate the variable that causes perfect prediction.

2.  Take $\hat\beta$ instead of $\dot\beta$.

3.  Use penalized regression with Jeffreys prior in step 2 of
    Algorithm \[alg:binary\] [@FIRTH1993; @HEINZE2002].

4.  Use the bootstrap, and then apply method 1.

5.  Use data augmentation, a method that concatenates
    pseudo-observations with a small weight to the data, effectively
    prohibiting infinite estimates [@CLOGG1991; @WHITE2010B].

6.  Apply the explicit Bayesian sampling with a suitable
    weak prior. @GELMAN2008B recommend using independent Cauchy
    distributions on all logistic regression coefficients.

Eliminating the most predictive variable is generally undesirable in the
context of imputation, and may in fact bias the relation of interest.
Option 2 does not yield proper imputations, and is therefore not
recommended. Option 3 provides finite estimates, but has been criticized
as not being well interpretable in a regression context [@GELMAN2008B]
and computationally inefficient [@WHITE2010B]. Option 4 corrects method
1, and is simple to implement. Options 5 and 6 have been recommended by
@WHITE2010B and @GELMAN2008B, respectively.

Methods 4, 5 and 6 all solve a major difficulty in the construction of
automatic imputation techniques. It is not yet clear whether one of
these methods is superior. The `logreg`,
`polr` and `polyreg`
methods in `mice` implement
option 5.

### Evaluation

The methods are based on the elegant generalized linear models.
Simulations presented in @VANBUUREN2006 show that these methods
performed quite well in the lab. When used in practice however, the
methods may be unstable, slow and exhibit poor performance. @HARDT2013
intentionally pushed the logistic methods to their limits, and observed
that most methods break down relatively quick, i.e., if the proportion
of missing values exceeds 0.4. @VANDERPALM2016A found that
`logreg` failed to pick up a three-way
association in the data, leading to biased estimates. Likewise,
@VIDOTTO2015 observed that `logreg` did not
recover the structure in the data as well as latent class models.
@WU2015 found poor results for all three methods (i.e., binary,
multinomial and proportional odds), and advise against their
application. @AKANDE2017 reported difficulties with fitting multinomial
variables having many categories. The performance of the procedures
suffered when variables with probabilities nearly equal to one (or zero)
are included in the models. Methods based on the generalized linear
model were found to be inferior to method
`cart` (cf. Section \[sec:cart\]) and to
latent class models for categorical data (cf. Section \[sec:JM\]).
@AUDIGIER2017 found that logistic regression presented difficulties on
the datasets with a high number of categories, resulting in
undercoverage on several quantities.

Imputation of categorical data is more difficult than continuous data.
As a rule of thumb, in logistic regression we need at least *10 events
per predictor* in order to get reasonably stable estimates of the
regression coefficients [@BELLE2002 p. 87]. So if we impute 10 binary
outcomes, we need $100$ events, and if the events occur with a
probability of 0.1, then we need $n > 1000$ cases. If we impute outcomes
with more categories, the numbers rapidly increase for two reasons.
First, we have more possible outcomes, and we need 10 events for each
category. Second, when used as predictor, each nominal variable is
expanded into dummy variables, so the number of predictors multiplies by
the number of categories minus 1. The defaults
`logreg`, `polyreg`
and `polr` tend to preserve the main effects
well provided that the parameters are identified and can be reasonably
well estimated. In many datasets, especially those with many categories,
the ratio of the number of fitted parameters relative to the number of
events easily drops below 10, which may lead to estimation problems. In
those cases, the advice is to specify more robust methods, like
`pmm`, `cart` or
`rf`.

## Other data types

### Count data {#sec:count}

Examples of count data include the number of children in a family or the
number of crimes committed. The minimum value is zero. Imputing
incomplete count data should produce non-negative synthetic replacement
values. Count data can be imputed in various ways:

1.  Predictive mean matching (cf. Section \[sec:pmm\]).

2.  Ordered categorical imputation
    (cf. Section \[sec:categorical\]).

3.  (Zero-inflated) Poisson regression [@RAGHUNATHAN2001].

4.  (Zero-inflated) negative binomial regression
    [@ROYSTON2009].

Poisson regression is a class of models that is widely applied in
biostatistics. The Poisson model can be thought of as the sum of the
outcomes from a series of independent flips of the coin. The negative
binomial is a more flexible model that is often applied an as
alternative to account for over-dispersion. Zero-inflated versions of
both models can be used if the number of zero values is larger than
expected. The models are special cases of the generalized linear model,
and do not bring new issues compared to, say, logistic regression
imputation.

@KLEINKE2013 developed methods for zero-inflated and over-dispersed
data, using both Bayesian and bootstrap approaches. Methods are
available in the `countimp` package for the
Poisson model (`pois`,
`pois.boot`), quasi Poission model
(`qpois`,
`qpois.boot`), the negative binomial model
(`nb`), the zero-inflated Poisson
(`2l.zip`,
`2l.zip.boot`), and the zero-inflated negative
binomial (`2l.zinb`,
`2l.zinb.boot`). Note that, despite their
naming, these `2l` methods are for
single-level imputation. An alternative is the
`ImputeRobust` package [@SALFRAN2017], which
implements the following `mice` methods for
count data: `gamlssPO` (Poisson),
`gamlssZIBI` (zero-inflated binomial) and
`gamlssZIP` (zero-inflated Poisson).
@KLEINKE2017 evaluated the use of predictive mean matching as a
multipurpose missing data tool. By and large, the simulations illustrate
that the method is robust against violations of its assumptions, and can
be recommended for imputation of mildly to moderately skewed variables
when sample size is sufficiently large.

### Semi-continuous data {#sec:semi}

Semi-continuous data have a high mass at one point (often zero) and a
continuous distribution over the remaining values. An example is the
number of cigarettes smoked per day, which has a high mass at zero
because of the non-smokers, and an often highly skewed unimodal
distribution for the smokers. The difference with count data is gradual.
Semi-continuous data are typically treated as continuous data, whereas
count data are generally considered discrete.

Imputation of semi-continuous variables needs to reproduce both the
point mass and continuously varying part of the data. One possibility is
to apply a general-purpose method that preserves distributional
features, like predictive mean matching (cf. Section \[sec:pmm\]).

An alternative is to model the data in two parts. The first step is to
determine whether the imputed value is zero or not. The second step is
only done for those with a non-zero value, and consists of drawing a
value from the continuous part. @OLSEN2001 developed an imputation
technique by combining a logistic model for the discrete part, and a
normal model for the continuous part, possibly after a normalizing
transformation. A more general two-part model was developed by
@JAVARAS2003, who extended the standard general location model
[@OLKIN1961] to impute partially observed semi-continuous data.

@YU2007 evaluated nine different procedures. They found that predictive
mean matching performs well, provided that a sufficient number of data
points in the neighborhood of the incomplete data are available.
@VINK2014B found that generic predictive mean matching is at least as
good as three dedicated methods for semi-continuous data: the two-part
models as implemented in `mi` [@SU2011] and
`irmi` [@TEMPL2011], and the blocked general
location model by @JAVARAS2003. @VROOMEN2016 investigated imputation of
cost data, and found that predictive mean matching of the
log-transformed outperformed plain predictive mean matching, a two-step
method and complete-case analysis, and hence recommend log-transformed
method for monetary data.

### Censored, truncated and rounded data {#sec:censored}

An observation $y_i$ is censored if its value is only partly known. In
*right-censored* data we only know that $y_i > a_i$ for a censoring
point $a_i$. In *left-censored* data we only know that $y_i \leq b_i$
for some known censoring point $b_i$, and in *interval censoring* we
know $a_i \leq y_i \leq
b_i$. Right-censored data arise when the true value is beyond the
maximum scale value, for example, when body weight exceeds the scale
maximum, say 150kg. When $y_i$ is interpreted as time taken to some
event (e.g., death), right-censored data occur when the observation
period ends before the event has taken place. Left and right censoring
may cause floor and ceiling effects. Rounding data to fewer decimal
places results in interval-censored data.

Truncation is related to censoring, but differs from it in the sense
that value below (left truncation) or above (right truncation) the
truncation point is not recorded at all. For example, if persons with a
weight in excess of 150kg are removed from the sample, we speak of
truncation. The fact that observations are entirely missing turns the
truncation problem into a missing data problem. Truncated data are less
informative than censored data, and consequently truncation has a larger
potential to distort the inferences of interest.

The usual approach for dealing with missing values in censored and
truncated data is to delete the incomplete records, i.e., complete-case
analysis. In the event that time is the censored variable, consider the
following two problems:

-   *Censored event times*. What would have been the uncensored event
    time if no censoring had taken place?

-   *Missing event times*. What would have been the event time and the
    censoring status if these had been observed?

The problem of censored event times has been studied extensively. There
are many statistical methods that can analyze left- or right-censored
data directly, collectively known as *survival analysis*.
@KLEINBAUM2005, @HOSMER2008 and @ALLISON2010 provide useful
introductions into the field. Survival analysis is the method of choice
if censoring is restricted to the single outcomes. The approach is,
however, less suited for censored predictors or for multiple
interdependent censored outcomes. @VANWOUWE2009 discuss an empirical
example of such a problem. The authors are interested in knowing time
interval between resuming contraception and cessation of lactation in
young mothers who gave birth in the last 6 months. As the sample was
cross-sectional, both contraception and lactation were subject to
censoring. Imputation could be used to impute the hypothetically
uncensored event times in both durations, and this allowed a study of
the association between the uncensored event times.

The problem of missing event times is relevant if the event time is
unobserved. The censoring status is typically also unknown if the event
time is missing. Missing event times may be due to happenstance, for
example, resulting from a technical failure of the instrument that
measures event times. Alternatively, the missing data could have been
caused by truncation, where all event times beyond the truncation point
are set to missing. It will be clear that the optimal way to deal with
the missing events data depends on the reasons for the missingness.
Analysis of the complete cases will systematically distort the analysis
of the event times if the data are truncated.

Imputation of right-censored data has received most attention to date.
In general, the method aims to find new (longer) event times that would
have been observed had the data not been censored. Let $n_1$ denote the
number of observed failure times, let $n_0=n-n_1$ denote the number of
censored event times and let ${t_1,\dots,t_n}$ be the ordered set of
failure and censored times. For some time point $t$, the *risk set*
$R(t) = t_i>t$ for $i=1,\dots,n$ is the set of event and censored times
that is longer than $t$. @TAYLOR2002 proposed two imputation strategies
for right-censored data:

1.  *Risk set imputation*. For a given censored value $t$
    construct the risk set $R(t)$, and randomly draw one case from
    this set. Both the failure time and censoring status from the
    selected case are used to impute the data.

2.  *Kaplan–Meier imputation*. For a given censored value $t$
    construct the risk set $R(t)$ and estimate the Kaplan–Meier curve
    from this set. A randomly drawn failure time from the Kaplan–Meier
    curve is used for imputation.

Both methods are asymptotically equivalent to the Kaplan–Meier estimator
after multiple imputation with large $m$. The adequacy of imputation
procedures will depend on the availability of possible donor
observations, which diminishes in the tails of the survival
distribution. The Kaplan–Meier method has the advantage that nearly all
censored observations are replaced by imputed failure times. In
principle, both Bayesian sampling and bootstrap methods can be used to
incorporate model uncertainty, but in practice only the bootstrap has
been used.

@HSU2006 extended both methods to include covariates. The authors fitted
a proportional hazards model and calculated a risk score as a linear
combination of the covariates. The key adaptation is to restrict the
risk set to those cases that have a risk score that is similar to the
risk score of censored case, an idea similar to predictive mean
matching. A donor group size with $d=10$ was found to perform well, and
Kaplan–Meier imputation was superior to risk set imputation across a
wide range of situations.

Algorithm \[alg:KMIB\] is based on the KIMB method proposed by @HSU2006.
The method assumes that censoring status is known, and aims to impute
plausible event times for censored observations. @HSU2006 actually
suggested fitting two proportional hazards models, one with survival
time as outcome and one with censoring status as outcome, but in order
to keep in line with the rest of this chapter, here we only fit the
model for survival time. The way in which predictive mean matching is
done differs slightly from @HSU2006.

The literature on imputation methods for censored and rounded data is
rapidly evolving. Alternative methods for right-censored data have also
been proposed [@WEI1991; @GESKUS2001; @LAM2005; @LIU2011]. @LYLES2001,
@LYNN2001, @HOPKE2001 and @LEE2018 concentrated on left-censored data.
Imputation of interval-censored data (rounded data) has been discussed
quite extensively
[@HEITJAN1990; @DOREY1993; @JAMES1995; @PAN2000; @BEBCHUK2000; @GLYNN2004; @HSU2007; @ROYSTON2007; @CHEN2010; @HSU2015].
Imputation of double-censored data, where both the initial and the final
times are interval censored, is treated by @PAN2001 and @ZHANG2009.
@DELORD2016 extended Pan’s approach to interval-censored competing risks
data, thus allowing estimation of the survival function, cumulative
incidence function, Cox and Fine & Gray regression coefficients. These
methods are available in the `MIICD` package.
@JACKSON2014 used multiple imputation to study departures from the
independent censoring assumption in the Cox model.

By comparison, very few methods have been developed to deal with
truncation. Methods for imputing a missing censoring indicator have been
proposed by @SUBRAMANIAN2009 [@SUBRAMANIAN2011] and @WANG2010.

## Nonignorable missing data {#sec:nonignorable}

### Overview {#sec:nonignorableoverview}

All methods described thus far assume that the missing data mechanism is
ignorable. In this case, there is no need for an explicit model of the
missing data process (cf. Section \[sec:ignorability\]). In reality, the
mechanism may be nonignorable, even after accounting for any measurable
factors that govern the response probability. In such cases, we can try
to adapt the imputed data to make them more realistic. Since such
adaptations are based on unverifiable assumptions, it is recommended to
study carefully the impact of different possibilities on the final
inferences by means of sensitivity analysis.

When is the assumption of ignorability suspect? It is hard to provide
cut-and-dried criteria, but the following list illustrates some typical
situations:

-   If important variables that govern the missing data process are not
    available;

-   If there is reason to believe that responders differ from
    non-responders, even after accounting for the observed information;

-   If the data are truncated.

If ignorability does not hold, we need to model the distribution
$P(Y,R)$ instead of $P(Y)$. For nonignorable missing data mechanisms,
$P(Y,R)$ do not factorize into independent parts. Two main strategies to
decompose $P(Y,R)$ are known as the *selection model* [@HECKMAN1976] and
the *pattern-mixture model* [@GLYNN1986B]. @LITTLE2002 [ch. 15] and
@LITTLE2009 provide in-depth discussions of these models.

Imputations are created most easily under the pattern-mixture model.
@HERZOG1983 [pp. 222–224] proposed a simple and general family of
nonignorable models that accounts for shift bias, scale bias and shape
bias. Suppose that we expect that the nonrespondent data are shifted
relative to the respondent data. Adding a simple shift parameter
$\delta$ to the imputations creates a difference in the means of a
$\delta$. In a similar vein, if we suspect that the nonrespondents and
respondents use different scales, we can multiply each imputation by a
scale parameter. Likewise, if we suspect that the shapes of both
distributions differ, we could redraw values from the candidate
imputations with a probability proportional to the dissimilarity between
the two distributions, a technique known as the SIR algorithm
[@RUBIN1987B]. We only discuss the shift parameter $\delta$.

In practice, it may be difficult to specify the distribution of the
nonrespondents, e.g., to provide a sensible specification of $\delta$.
One approach is to compare the results under different values of
$\delta$ by sensitivity analysis. Though helpful, this puts the burden
on the specification of realistic scenarios, i.e., a set of plausible
$\delta$-values. The next sections describe the selection model and
pattern mixture in more detail, as a way to evaluate the plausibility of
$\delta$.

### Selection model {#sec:selectionmodel}

The selection model [@HECKMAN1976] decomposes the joint distribution
$P(Y,R)$ as $$P(Y,R) = P(Y)P(R|Y). \label{eq:selection}$$ The selection
model multiplies the marginal distribution $P(Y)$ in the population with
the response weights $P(R|Y)$. Both $P(Y)$ and $P(R|Y)$ are unknown, and
must be specified by the user. The model where $P(Y)$ is normal and
where $P(R|Y)$ is a probit model is known as the Heckman model. This
model is widely used in economics to correct for selection bias.

lrrrr $Y$&$P(Y)$&$P(R=1|Y)$&$P(Y|R=1)$&$P(Y|R=0)$\
100&0.02&0.65&0.015&0.058\
110&0.03&0.70&0.024&0.074\
120&0.05&0.75&0.043&0.103\
130&0.10&0.80&0.091&0.164\
140&0.15&0.85&0.145&0.185\
150&0.30&0.90&0.307&0.247\
160&0.15&0.92&0.157&0.099\
170&0.10&0.94&0.107&0.049\
180&0.05&0.96&0.055&0.016\
190&0.03&0.98&0.033&0.005\
200&0.02&1.00&0.023&0.000\
\
$\bar Y$&150.00&&151.58&138.60\

*Numerical example*. The column labeled $Y$ in Table \[tab:c85nmar\]
contains the midpoints of 11 categories of systolic blood pressure. The
column $P(Y)$ contains a hypothetically complete distribution of
systolic blood pressure. It is specified here as symmetric with a mean
of 150mmHg (millimeters mercury). This distribution should be a
realistic description of the *combined* observed and missing blood
pressure values in the population of interest. The column $P(R=1|Y)$
specifies the probability that blood pressure is actually observed at
different levels of blood pressure. Thus, at a systolic blood pressure
of 100mmHg, we expect that 65% of the data will be observed. On the
other hand, we expect that no missing data occur for those with a blood
pressure of 200mmHg. This specification produces 12.2% of missing data.
The variability in the missingness probability is large, and reflects an
extreme scenario where the missing data are created mostly at the lower
blood pressures. Section \[sec:c85causes\] discusses why more missing
data in the lower levels are plausible. When taken together, the columns
$P(Y)$ and $P(R=1|Y)$ specify a selection model.

### Pattern-mixture model {#sec:patternmixturemodel}

The pattern-mixture model [@GLYNN1986B; @LITTLE1993] decomposes the
joint distribution $P(Y,R)$ as $$\begin{aligned}
  P(Y,R)&=& P(Y|R)P(R)\\
  &=&P(Y|R=1)P(R=1) + P(Y|R=0)P(R=0)\end{aligned}$$ Compared to Equation
\[eq:selection\] this model only reverses the roles of $Y$ and $R$, but
the interpretation is quite different. The pattern-mixture model
emphasizes that the combined distribution is a mix of the distributions
of $Y$ in the responders and nonresponders. The model needs a
specification of the distribution $P(Y|R=1)$ of the responders (which
can be conveniently modeled after the data), and of the distribution
$P(Y|R=0)$ of the nonresponders (for which we have no data at all). The
joint distribution is the mixture of these two distributions, with
mixing probabilities $P(R=1)$ and $P(R=0)=1-P(R=1)$, the overall
proportions of observed and missing data, respectively.

*Numerical example*. The columns labeled $P(Y|R=1)$ and $P(Y|R=0)$ in
Table \[tab:c85nmar\] contain the probability per blood pressure
category for the respondents and nonrespondents. Since more missing data
are expected to occur at lower blood pressures, the mass of the
nonresponder distribution has shifted toward the lower end of the scale.
As a result, the mean of the nonresponder distribution is equal to
138.6mmHg, while the mean of the responder distribution equals
151.58mmHg.

### Converting selection and pattern-mixture models {#sec:convert}

The pattern-mixture model and the selection model are connected via
Bayes rule. Suppose that we have a mixture model specified as the
probability distributions $P(Y|R=0)$ and $P(Y|R=1)$ plus the overall
response probability $P(R)$. The corresponding selection model can be
calculated as $$P(R=1|Y=y) = P(Y=y|R=1)P(R=1) / P(Y=y)$$ where the
marginal distribution of $Y$ is
$$P(Y=y) = P(Y=y|R=1)P(R=1) + P(Y=y|R=0)P(R=0)$$

Reversely, the pattern-mixture model can be calculated from the
selection model as follows: $$P(Y=y|R=r) = P(R=r|Y=y)P(Y=y) / P(R=r)$$
where the overall probability of observed ($r=1$) or missing ($r=0$)
data is equal to $$P(R=r) = \sum_y P(R=r|Y=y)P(Y=y)$$

*Numerical example*. In Table \[tab:c85nmar\] we calculate
$P(Y=100) = 0.015 \times 0.878 + 0.058 \times 0.122 = 0.02$. Likewise,
we find $P(R=1|Y) = 0.015 \times 0.878 / 0.02 = 0.65$ and $P(R=0|Y) =
0.058 \times 0.122/0.02 = 0.35$. The reverse calculation is left as an
exercise to the reader.

![Graphic representation of the response mechanism for systolic blood
pressure in Table \[tab:c85nmar\]. See text for explanation.<span
data-label="c85:fignmar">](fig/ch3_c85sensfig-1){width="\maxwidth"}

Figure \[c85:fignmar\] is an illustration of the posited missing data
mechanism. The left-hand figure displays the missingness probabilities
$P(R|Y)$ of the selection model. The right-hand plot provides the
distributions $P(Y|R)$ in the observed (gray) and missing
(red) data in the corresponding pattern-mixture model. The
hypothetically complete distribution is given by the black curve. The
distribution of blood pressure in the group with missing blood pressures
is quite different, both in form and location. At the same time, observe
that the effect of missingness on the combined distribution is only
slight. The reason is that 87% of the information is actually observed.

The mean of the distribution of the observed data remains almost
unchanged (151.6mmHg instead of 150mmHg), but the mean of the
distribution of the missing data is substantially lower at 138.6mmHg.
Thus, under the assumed selection model we expect that the mean of the
imputed data should be $151.6-138.6 = 13$mmHg lower than in the observed
data.

### Sensitivity analysis {#sec:ch3sensitivity}

Sections \[sec:selectionmodel\]–\[sec:convert\] provide different,
though related, views on the assumed response model. A fairly extreme
response model where the missingness probability increases from 0% to
35% in the outcome produces a mean difference of 13mmHg. The effect in
the combined distribution is much smaller: 1.6mmHg.

     $\delta$    Interpretation
  ----------- -- --------------------------
        0mmHg    MCAR, $\delta$ too small
     $-$5mmHg    Small effect
    $-10$mmHg    Large effect
    $-$15mmHg    Extreme effect
    $-$20mmHg    Too extreme effect

  : Difference between the means of the blood pressure distributions of
  the response and nonresponse groups, and its interpretation in the
  light of what we know about the data.<span
  data-label="tab:delta">

Section \[sec:nonignorableoverview\] discussed the idea of adding some
extra mmHg to the imputed values, a method known as $\delta$-adjustment.
It is important to form an idea of what reasonable values for $\delta$
could be. Under the posited model, $\delta=0$mmHg is clearly too small
(as it assumes MCAR), whereas $\delta=-20$mmHg is too extreme (as it can
only occur if nearly all missing values occur in the lowest blood
pressures). Table \[tab:delta\] provides an interpretation of various
values for $\delta$. The most likely scenarios would yield $\delta=-5$
or $\delta=-10$mmHg.

In practice, part of $\delta$ may be realized through the predictors
needed under MAR. It is useful to decompose $\delta$ as
$\delta = \delta_\mathrm{MAR} + \delta_\mathrm{MNAR}$, where
$\delta_\mathrm{MAR}$ is the mean difference caused by the predictors in
the imputation models, and where $\delta_\mathrm{MNAR}$ is the mean
difference caused by an additional nonignorable part of the imputation
model. If candidate imputations are produced under MAR, we only need to
add a constant $\delta_\mathrm{MNAR}$. Section \[sec:sensitivity\]
continues this application.

Adding a constant may seem overly simple, but it is actually quite
powerful. In cases where no one model will be obviously more realistic
than any other, @RUBIN1987 [p. 203] stressed the need for easily
communicated models, like a “20% increase over the ignorable value.”
@LITTLE2009 [p. 49] warned that it is easy to be enamored of complicated
models for $P(Y,R)$ so that we may be “lulled into a false sense of
complacency about the fundamental lack of identification,” and suggested
simple methods:

> The idea of adding offsets is simple, transparent, and can be readily
> accomplished with existing software.

Adding a constant or multiplying by a value are in fact the most direct
ways to specify nonignorable models.

### Role of sensitivity analysis

Nonignorable models are only useful after the possibilities to make the
data “more MAR” have been exhausted. A first step is always to create
the best possible imputation model based on the available data. Section
\[sec:predictors\] provides specific advice on how to build imputation
models.

The MAR assumption has been proven defensible for intentional missing
data. In general, however, we can never rule out the possibility that
the data are MNAR. In order to cater for this possibility, many advise
performing a sensitivity analysis on the final result. This is voiced
most clearly in recommendation 15 of the National Research Council’s
advice on clinical trials [@NRC2010]:

> Recommendation 15: Sensitivity analysis should be part of the primary
> reporting of findings from clinical trials. Examining sensitivity to
> the assumptions about the missing data mechanism should be a mandatory
> component of reporting.

While there is much to commend this rule, we should refrain from doing
sensitivity analysis just for the sake of it. The proper execution of a
sensitivity analysis requires us to specify plausible scenarios. An
extreme scenario like “suppose that all persons who leave the study die”
can have a major impact on the study result, yet it could be highly
improbable and therefore of limited interest.

Sensitivity analysis on factors that are already part of the imputation
model is superfluous. Preferably, before embarking on a sensitivity
analysis, there should be reasonable evidence that the MAR assumption is
(still) inadequate after the available data have been taken into
account. Such evidence is also crucial in formulating plausible MNAR
mechanisms. Any decisions about scenarios for sensitivity analysis
should be taken in discussion with subject-matter specialists. There is
no purely statistical solution to the problem of nonignorable missing
data. Sensitivity analysis can increase our insight into the stability
of the results, but in my opinion we should only use it if we have a
firm idea of which scenarios for the missingness would be reasonable.

In practice, we may lack such insights. In such instances, I would
prefer a carefully constructed imputation model (which is based on all
available data) over a poorly constructed sensitivity analysis.

### Recent developments

The literature on nonignorable models is large and diverse. The research
in this area is active and recent. This section provides some pointers
into the recent literature.

The historic overview by @KENWARD2015 provides an in-depth treatment of
the selection, pattern-mixture and shared parameter models, including
their connnections. The Handbook of Missing Data Methodology
[@MOLENBERGHS2015] contains five chapters that discuss sensitivity
analysis from all angles. The handbook should be the starting point for
anyone considering models for data that are MNAR. @LITTLE2017 developed
an alternative strategy based on selecting a subset of parameters of
substantive interest. In particular cases, the conditions for ignoring
the missing-data mechanism are more relaxed than under MAR.

Regulators prefer simple methods that impute the missing outcomes under
MAR, and then add an adjustment $\delta$ to the imputes, while varying
$\delta$ over a plausible range and independently for each treatment
group [@PERMUTT2016]. The most interesting scenarios will be those where
the difference between the $\delta$’s correspond to the size of the
treatment effect in the completers. Contours of the $p$-values may be
plotted on a graph as a function of the $\delta$’s to assist in a
tipping-point analysis [@LIUBLINSKA2014].

@KACIROTO2014 relate the identifying parameters from the pattern-mixture
model to the corresponding missing data mechanism in the selection
model. This dual interpretation provides a unified framework for
performing sensitivity analysis. @GALIMARD2016 proposed an imputation
method under MNAR based on the Heckman model. The random indicator
method [@JOLANI2012] is an experimental iterative method that redraws
the missing data indicator under a selection model, and imputes the
missing data under a pattern-mixture model, with the objective of
estimating $\delta$ from the data under relaxed assumptions. Initial
simulation results look promising. The algorithm is available as the
`ri` method in
`mice`.

## Exercises {#ex:ch:univariate}

1.  *MAR*. Reproduce Table \[tab:linmody\] and Table
    \[tab:linmodx\] for MARRIGHT, MARMID and MARTAIL missing data
    mechanisms of Section \[sec:generateuni\].

    1.  Are there any choices that you need to make? If so,
        which?

    2.  Consider the six possibilities to combine the missing data
        mechanism and missingness in $x$ or $y$. Do you expect
        complete-case analysis to perform well in each case?

    3.  Do the Bayesian sampling and bootstrap methods also work
        under the three MAR mechanisms?

    \[ex:marlin\]

2.  *Parameter uncertainty*. Repeat the simulations of Section
    \[sec:linearnormal\] on the `whiteside`
    data for different samples sizes.

    1.  Use the method of Section \[sec:perflin\] to generate an
        artificial population of 10000 synthetic gas
        consumption observations. Re-estimate the parameter from the
        artificial population. How close are they to the “true”
        values?

    2.  Draw random samples from the artificial population.
        Systematically vary sample size. Is there some sample size at
        which `norm.nob` is as good as the
        Bayesian sampling and bootstrap methods?

    3.  Is the result identical for missing $y$ and missing
        $x$?

    4.  Is the result the same after including insulation status
        in the model?

    \[ex:sampling\]
